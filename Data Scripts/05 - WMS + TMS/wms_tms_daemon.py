#!/usr/bin/env python3
"""
GenIMS WMS + TMS Daemon - ULTRA FAST MODE
Generates complete warehouse and logistics operations in-memory, then bulk dumps to PostgreSQL
"""

import sys
import os
import time
import logging
import signal
from datetime import datetime, timedelta
import random
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

env_file = os.path.join(os.path.dirname(__file__), '..', '..', 'scripts', 'config.env')
if os.path.exists(env_file):
    load_dotenv(env_file)

try:
    import psycopg2
    from psycopg2.extras import execute_batch
    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False

# Configuration
PG_HOST = os.getenv('POSTGRES_HOST', 'localhost')
PG_PORT = int(os.getenv('POSTGRES_PORT', '5432'))
PG_USER = os.getenv('POSTGRES_USER', 'postgres')
PG_PASSWORD = os.getenv('POSTGRES_PASSWORD', '')
PG_SSL_MODE = os.getenv('PG_SSL_MODE', 'require')

PG_MASTER_DB = os.getenv('DB_MASTER', 'genims_master_db')
PG_WMS_DB = os.getenv('DB_WMS', 'genims_wms_db')
PG_TMS_DB = os.getenv('DB_TMS', 'genims_tms_db')

WMS_BATCH_SIZE = 5000
TMS_BATCH_SIZE = 5000
# Aligned with ERP volume (150-300 sales orders/day) and warehouse capacity (12 warehouses)
# WMS: Inbound (80-150/day) + Outbound waves (60-120/day) + picking tasks (~200-400/day) = ~400-700 total
WMS_TOTAL_RECORDS = 550  # Daily WMS operations (receiving + waves + picking tasks)
# TMS: Shipments (~20-40/day) + Tracking events (~80-160/day) + Routes (~15-30/day) = ~120-240 total  
TMS_TOTAL_RECORDS = 180  # Daily TMS operations (aligned with sales order volume)

# Logging
log_dir = os.getenv('DAEMON_LOG_DIR', os.path.join(os.path.dirname(__file__), '..', '..', 'logs'))
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, 'wms_tms_daemon.log')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('WMSTMSDaemon')

# Global State
pg_wms_connection = None
pg_tms_connection = None
pg_erp_connection = None
master_data = {}
carrier_ids = []
valid_material_ids = set()
valid_sales_order_ids = set()
sim_base_time = datetime.now()  # Coordinated timestamp
counters = {'wave': 1, 'receiving_task': 1, 'picking_task': 1, 'packing_task': 1, 'shipping_task': 1,
            'sales_order': 1, 'shipment': 1, 'tracking_event': 1, 'delivery': 1, 'route': 1}

# Statistics
stats = {
    'wms_tasks_created': 0,
    'tms_shipments_created': 0,
    'tms_events_created': 0,
    'tms_deliveries_created': 0,
    'tms_routes_created': 0,
    'fk_validation_errors': 0,
    'inventory_inconsistencies': 0,
    'start_time': datetime.now()
}

def signal_handler(sig, frame):
    logger.info("Shutdown signal received")
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

def _get_max_timestamp():
    """Get maximum timestamp from WMS+TMS transactions and prevent future dates"""
    global sim_base_time
    try:
        if not pg_wms_connection or not pg_tms_connection:
            logger.warning("No database connection available for timestamp check")
            sim_base_time = datetime.now()
            return
        
        wms_cursor = pg_wms_connection.cursor()
        tms_cursor = pg_tms_connection.cursor()
        
        # Get max from key WMS tables
        wms_cursor.execute("""
            SELECT MAX(greatest_timestamp) FROM (
                SELECT MAX(created_at::timestamp) as greatest_timestamp FROM pick_waves
                UNION ALL
                SELECT MAX(created_at::timestamp) FROM receiving_tasks  
                UNION ALL
                SELECT MAX(created_at::timestamp) FROM shipping_tasks
                UNION ALL
                SELECT MAX(created_at::timestamp) FROM picking_tasks
            ) combined_wms_timestamps
        """)
        
        wms_max = wms_cursor.fetchone()[0]
        
        # Get max from key TMS tables
        tms_cursor.execute("""
            SELECT MAX(greatest_timestamp) FROM (
                SELECT MAX(created_at::timestamp) as greatest_timestamp FROM shipments
                UNION ALL
                SELECT MAX(event_timestamp::timestamp) FROM tracking_events
                UNION ALL
                SELECT MAX(actual_delivery_date::timestamp) FROM deliveries
            ) combined_tms_timestamps
        """)
        
        tms_max = tms_cursor.fetchone()[0]
        
        wms_cursor.close()
        tms_cursor.close()
        
        # Use the latest timestamp from both systems
        max_timestamp = None
        if wms_max and tms_max:
            max_timestamp = max(wms_max, tms_max)
        elif wms_max:
            max_timestamp = wms_max
        elif tms_max:
            max_timestamp = tms_max
        
        if max_timestamp:
            if isinstance(max_timestamp, str):
                max_ts = datetime.strptime(max_timestamp, '%Y-%m-%d %H:%M:%S')
            else:
                max_ts = max_timestamp
            
            # CRITICAL: Never use future dates
            current_date = datetime.now()
            if max_ts > current_date:
                logger.warning(f"Found future timestamp {max_ts}, using current date instead")
                max_ts = current_date
            
            # Set to next operational period
            next_period = max_ts + timedelta(hours=1)
            sim_base_time = next_period.replace(minute=0, second=0, microsecond=0)
            
            # Double-check: if next period is still future, use current time
            if sim_base_time > current_date:
                logger.warning(f"Next period {sim_base_time} is in future, using current time instead")
                sim_base_time = current_date.replace(minute=0, second=0, microsecond=0)
        else:
            logger.info("No existing WMS/TMS timestamps found, using current time")
            sim_base_time = datetime.now().replace(minute=0, second=0, microsecond=0)
        
        logger.info(f"WMS/TMS simulation will start from: {sim_base_time}")
        
    except Exception as e:
        logger.error(f"Error getting max timestamp: {e}")
        sim_base_time = datetime.now().replace(minute=0, second=0, microsecond=0)

def validate_foreign_keys():
    """Validate critical foreign key relationships without schema constraints"""
    global valid_material_ids, valid_sales_order_ids
    
    try:
        # Connect to ERP database for FK validation
        global pg_erp_connection
        pg_erp_connection = psycopg2.connect(
            host=PG_HOST, port=PG_PORT, database=os.getenv('DB_ERP', 'genims_erp_db'),
            user=PG_USER, password=PG_PASSWORD, sslmode=PG_SSL_MODE, connect_timeout=30
        )
        
        erp_cursor = pg_erp_connection.cursor()
        
        # Load valid material IDs
        erp_cursor.execute("SELECT material_id FROM materials WHERE material_status = 'active'")
        valid_material_ids = set(row[0] for row in erp_cursor.fetchall())
        
        # Load valid sales order IDs
        erp_cursor.execute("SELECT sales_order_id FROM sales_orders")
        valid_sales_order_ids = set(row[0] for row in erp_cursor.fetchall())
        
        erp_cursor.close()
        
        logger.info(f"Loaded FK validation sets: {len(valid_material_ids)} materials, {len(valid_sales_order_ids)} sales orders")
        
    except Exception as e:
        logger.warning(f"FK validation setup failed: {e}, generating fallback validation sets")
        # Create fallback validation sets using master data
        try:
            # Use materials from master data as fallback
            if 'materials' in master_data:
                valid_material_ids = set(mat['material_id'] for mat in master_data['materials'])
                logger.info(f"Using fallback: {len(valid_material_ids)} materials from master data")
            else:
                valid_material_ids = set()
            
            # Generate fallback sales order IDs
            valid_sales_order_ids = set(f"SO-{i:06d}" for i in range(1, 1001))  # 1000 fallback sales orders
            logger.info(f"Generated fallback: {len(valid_sales_order_ids)} sales order IDs")
            
        except Exception as fallback_error:
            logger.warning(f"Fallback validation failed: {fallback_error}, disabling FK validation")
            valid_material_ids = set()
            valid_sales_order_ids = set()

def validate_inventory_consistency(material_id: str, warehouse_id: str, quantity_change: float):
    """Check inventory consistency before operations (simulation-friendly)"""
    try:
        # In simulation mode, use simplified inventory tracking
        inventory_key = f"{warehouse_id}_{material_id}"
        
        # Initialize inventory tracker if not exists
        if not hasattr(validate_inventory_consistency, '_inventory_cache'):
            validate_inventory_consistency._inventory_cache = {}
        
        # Get current simulated inventory (start with reasonable amount)
        current_inventory = validate_inventory_consistency._inventory_cache.get(inventory_key, 1000.0)
        
        # Check if operation would create negative inventory
        if quantity_change < 0 and abs(quantity_change) > current_inventory:
            # Instead of failing, replenish inventory (simulate receiving)
            replenish_amount = abs(quantity_change) * 2  # Replenish 2x what's needed
            validate_inventory_consistency._inventory_cache[inventory_key] = current_inventory + replenish_amount
            logger.debug(f"Auto-replenished {material_id} in {warehouse_id}: +{replenish_amount}")
            return True
        
        # Update inventory cache
        validate_inventory_consistency._inventory_cache[inventory_key] = current_inventory + quantity_change
        return True
        
    except Exception as e:
        logger.debug(f"Inventory validation error: {e}, allowing operation")
        return True  # Allow operation to continue in case of errors

def initialize_database():
    global pg_wms_connection, pg_tms_connection
    if not POSTGRES_AVAILABLE:
        return False
    try:
        pg_wms_connection = psycopg2.connect(
            host=PG_HOST, port=PG_PORT, database=PG_WMS_DB,
            user=PG_USER, password=PG_PASSWORD, sslmode=PG_SSL_MODE, connect_timeout=30
        )
        pg_wms_connection.autocommit = False
        logger.info(f"PostgreSQL WMS connection established: {PG_HOST}:{PG_PORT}/{PG_WMS_DB}")
        
        pg_tms_connection = psycopg2.connect(
            host=PG_HOST, port=PG_PORT, database=PG_TMS_DB,
            user=PG_USER, password=PG_PASSWORD, sslmode=PG_SSL_MODE, connect_timeout=30
        )
        pg_tms_connection.autocommit = False
        logger.info(f"PostgreSQL TMS connection established: {PG_HOST}:{PG_PORT}/{PG_TMS_DB}")
        
        # Initialize time coordination and FK validation
        _get_max_timestamp()
        validate_foreign_keys()
        
        return True
    except Exception as e:
        logger.error(f"PostgreSQL connection failed: {e}")
        return False

def get_table_count(table_name, database=None):
    if database is None:
        database = PG_WMS_DB
    try:
        conn = psycopg2.connect(host=PG_HOST, port=PG_PORT, database=database,
            user=PG_USER, password=PG_PASSWORD, sslmode=PG_SSL_MODE, connect_timeout=10)
        cursor = conn.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
        count = cursor.fetchone()[0]
        cursor.close()
        conn.close()
        return count
    except Exception as e:
        logger.warning(f"Could not get {table_name} count: {e}")
        return None

def get_all_table_counts():
    wms_tables = ['pick_waves', 'receiving_tasks', 'picking_tasks', 'packing_tasks', 'shipping_tasks']
    tms_tables = ['shipments', 'tracking_events', 'deliveries', 'routes']
    counts = {}
    for table in wms_tables:
        counts[table] = get_table_count(table, PG_WMS_DB)
    for table in tms_tables:
        counts[table] = get_table_count(table, PG_TMS_DB)
    return counts

def load_master_data():
    global master_data, carrier_ids
    try:
        # Load warehouses from WMS database
        wms_conn = psycopg2.connect(
            host=PG_HOST, port=PG_PORT, database=PG_WMS_DB,
            user=PG_USER, password=PG_PASSWORD, sslmode=PG_SSL_MODE, connect_timeout=30
        )
        wms_cursor = wms_conn.cursor()
        
        wms_cursor.execute("SELECT warehouse_id FROM warehouses WHERE is_active = true LIMIT 10")
        warehouses = [row[0] for row in wms_cursor.fetchall()]
        wms_cursor.close()
        wms_conn.close()
        
        # Load carriers from TMS database (for shipments.carrier_id NOT NULL constraint)
        tms_conn = psycopg2.connect(
            host=PG_HOST, port=PG_PORT, database=PG_TMS_DB,
            user=PG_USER, password=PG_PASSWORD, sslmode=PG_SSL_MODE, connect_timeout=30
        )
        tms_cursor = tms_conn.cursor()
        
        tms_cursor.execute("SELECT carrier_id FROM carriers LIMIT 20")
        carrier_ids = [row[0] for row in tms_cursor.fetchall()]
        tms_cursor.close()
        tms_conn.close()
        
        # Load products from Folder 01 master data JSON (use as materials)
        import json
        folder_01_path = os.path.join(os.path.dirname(__file__), '..', '01 - Base Data', 'genims_master_data.json')
        master_data_path = os.path.abspath(folder_01_path)
        
        with open(master_data_path, 'r') as f:
            master_json = json.load(f)
        
        # Use products as materials, and get customers for delivery info
        products = master_json.get('products', [])
        customers = master_json.get('customers', [])
        
        materials = [{'material_id': p['product_id'], 'product_name': p.get('product_name', 'Unknown')} 
                     for p in products]
        
        master_data['warehouses'] = warehouses or ['WH-000001', 'WH-000002', 'WH-000003']
        master_data['materials'] = materials or []
        master_data['customers'] = customers or []
        
        logger.info(f"Master data loaded: {len(warehouses)} warehouses, {len(materials)} materials, {len(customers)} customers, {len(carrier_ids)} carriers")
        return True
    except Exception as e:
        logger.error(f"Failed to load master data: {e}")
        return False


def get_max_id_counter(table_name, id_column, connection):
    """Get the next ID counter for a given table"""
    try:
        cursor = connection.cursor()
        cursor.execute(f"SELECT MAX(CAST(SUBSTRING({id_column}, '\\d+$') AS INTEGER)) FROM {table_name}")
        max_id = cursor.fetchone()[0]
        cursor.close()
        return (max_id or 0) + 1
    except Exception as e:
        logger.debug(f"Could not get max ID from {table_name}.{id_column}: {e}")
        return 1

def initialize_id_counters():
    """Initialize counters from existing data in database"""
    global counters
    try:
        # WMS counters
        counters['wave'] = get_max_id_counter('pick_waves', 'wave_id', pg_wms_connection)
        counters['receiving_task'] = get_max_id_counter('receiving_tasks', 'receiving_task_id', pg_wms_connection)
        counters['picking_task'] = get_max_id_counter('picking_tasks', 'picking_task_id', pg_wms_connection)
        counters['packing_task'] = get_max_id_counter('packing_tasks', 'packing_task_id', pg_wms_connection)
        counters['shipping_task'] = get_max_id_counter('shipping_tasks', 'shipping_task_id', pg_wms_connection)
        
        # TMS counters
        counters['shipment'] = get_max_id_counter('shipments', 'shipment_id', pg_tms_connection)
        counters['tracking_event'] = get_max_id_counter('tracking_events', 'event_id', pg_tms_connection)
        counters['delivery'] = get_max_id_counter('deliveries', 'delivery_id', pg_tms_connection)
        counters['route'] = get_max_id_counter('routes', 'route_id', pg_tms_connection)
        
        # Sales order counter (synthetic - use max from any previous run or start at 1)
        counters['sales_order'] = 1
        
        logger.info(f"ID Counters initialized: {counters}")
        return True
    except Exception as e:
        logger.error(f"Failed to initialize ID counters: {e}")
        return False

def insert_batch_parallel(cursor, insert_sql, data, table_name, batch_size, connection=None, max_workers=3):
    """Insert data in parallel batches with sequential execution (PostgreSQL thread safety)"""
    total_batches = (len(data) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        try:
            batch_start = batch_idx * batch_size
            batch_end = min((batch_idx + 1) * batch_size, len(data))
            batch = data[batch_start:batch_end]
            
            execute_batch(cursor, insert_sql, batch, page_size=5000)
            if connection:
                connection.commit()
            
            logger.info(f"  Flushed {batch_end:,} / {len(data):,} {table_name}")
        except psycopg2.IntegrityError:
            logger.warning(f"  Batch {batch_idx + 1}/{total_batches} - Integrity error, skipping")
            if connection:
                connection.rollback()
        except Exception as e:
            logger.error(f"  Batch {batch_idx + 1}/{total_batches} error: {e}")
            if connection:
                connection.rollback()

def main():
    """Main - Generate all WMS + TMS data in-memory, then bulk dump"""
    logger.info("="*80)
    logger.info("GenIMS WMS + TMS Daemon - ULTRA FAST MODE (In-Memory Generation)")
    logger.info("="*80)
    logger.info(f"Configuration:")
    logger.info(f"  Databases: {PG_WMS_DB} (WMS), {PG_TMS_DB} (TMS)")
    logger.info(f"  WMS Batch Size: {WMS_BATCH_SIZE}")
    logger.info(f"  TMS Batch Size: {TMS_BATCH_SIZE}")
    logger.info("="*80)
    
    start_time = time.time()
    
    # Initialize
    if not initialize_database():
        return 1
    
    if not initialize_id_counters():
        return 1
    
    if not load_master_data():
        return 1
    
    logger.info("="*80)
    logger.info("ðŸ“Š BASELINE DATABASE COUNTS (Before Generation)")
    logger.info("="*80)
    counts_before = get_all_table_counts()
    for table, count in counts_before.items():
        if count is not None:
            logger.info(f"  {table:.<40} {count:>10,} records")
    logger.info("="*80)
    
    logger.info("="*80)
    logger.info("GENERATING ALL DATA IN MEMORY...")
    logger.info("="*80)
    
    # Generate WMS data
    pick_waves = []
    receiving_tasks = []
    picking_tasks = []
    packing_tasks = []
    shipping_tasks = []
    
    # Generate TMS data
    shipments = []
    tracking_events = []
    deliveries = []
    routes = []
    
    sim_base_time = datetime.strptime(datetime.now().strftime('%Y-%m-%d 00:00:00'), '%Y-%m-%d %H:%M:%S')
    run_timestamp = datetime.now().strftime('%Y%m%d%H%M%S')  # Unique per run
    
    # Generate WMS records with FK validation and realistic daily timing
    logger.info(f"Starting WMS generation: {WMS_TOTAL_RECORDS:,} records expected")
    for i in range(WMS_TOTAL_RECORDS):
        # Spread operations across 10-hour business day (8 AM - 6 PM)
        business_hours_minutes = 10 * 60  # 600 minutes
        minute_offset = (i * business_hours_minutes) // WMS_TOTAL_RECORDS
        current_ts = sim_base_time.replace(hour=8) + timedelta(minutes=minute_offset)
        
        warehouse_id = random.choice(master_data['warehouses'])
        
        # Use validated material IDs if available, otherwise fall back to master data
        if valid_material_ids and len(valid_material_ids) > 0:
            material_id = random.choice(list(valid_material_ids))
        else:
            material = random.choice(master_data['materials'])
            material_id = material['material_id']
        
        # FK Validation: Use fallback if material doesn't exist in ERP
        if valid_material_ids and len(valid_material_ids) > 0 and material_id not in valid_material_ids:
            # Use a valid material instead of skipping the record
            material_id = random.choice(list(valid_material_ids))
            # Only count as error in extreme cases
            if len(valid_material_ids) < 10:
                stats['fk_validation_errors'] += 1
        
        # Pick waves (daily target: 60-120 waves across all warehouses = ~15 per warehouse)
        # Generate wave every ~35-40 records to achieve ~15 waves per day
        if i % 35 == 0:
            wave_id = f"WAVE-{(counters['wave'] + i // 35):06d}"
            total_orders = random.randint(3, 8)  # Reduced from 3-10 for better distribution
            
            # Wave capacity check - limit concurrent waves
            wave_capacity_factor = min(1.0, 100.0 / total_orders)  # Reduce load if too many orders
            
            pick_waves.append({
                'wave_id': wave_id,
                'wave_number': f"WAVE-{run_timestamp}-{i // 35:04d}",
                'warehouse_id': warehouse_id,
                'wave_type': 'batch',
                'planned_pick_date': current_ts.date(),
                'priority': random.choice(['urgent', 'high', 'normal', 'low']),
                'wave_status': 'created',
                'total_orders': total_orders,
                'total_lines': int(total_orders * random.randint(2, 5) * wave_capacity_factor),
                'created_at': current_ts,
                'released_at': current_ts + timedelta(minutes=30),  # 30 min processing delay
                'completed_at': current_ts + timedelta(hours=4)     # 4 hour wave completion
            })
            stats['wms_tasks_created'] += 1
        
        # Receiving tasks (daily target: 80-150 tasks across all warehouses = ~25 per warehouse)  
        # Generate receiving every ~20 records to achieve ~25-30 receiving tasks per day
        if i % 20 == 0:
            task_id = f"RCV-{(counters['receiving_task'] + i // 20):06d}"
            expected_qty = random.randint(100, 1000)
            received_qty = random.randint(int(expected_qty * 0.8), expected_qty)  # 80-100% receipt rate
            
            receiving_tasks.append({
                'receiving_task_id': task_id,
                'task_number': f"RCV-{run_timestamp}-{i // 20:04d}",
                'warehouse_id': warehouse_id,
                'material_id': material_id,
                'expected_quantity': expected_qty,
                'received_quantity': received_qty,
                'unit_of_measure': 'EA',
                'task_status': 'completed',
                'receiving_dock': f"DOCK-{random.randint(1, 5)}",
                'created_at': current_ts,
                'completed_at': current_ts + timedelta(hours=2)
            })
            stats['wms_tasks_created'] += 1
        
        # Picking tasks (daily target: 200-400 tasks across all warehouses = ~80 per warehouse)
        # Generate picking every ~7 records to achieve ~80 picking tasks per day  
        if i % 7 == 0:
            task_id = f"PICK-{(counters['picking_task'] + i // 7):06d}"
            quantity_to_pick = random.randint(10, 100)
            
            # Inventory consistency check
            if validate_inventory_consistency(material_id, warehouse_id, -quantity_to_pick):
                quantity_picked = min(quantity_to_pick, random.randint(10, quantity_to_pick))
                
                picking_tasks.append({
                    'picking_task_id': task_id,
                    'task_number': f"PICK-{run_timestamp}-{i // 7:04d}",
                    'warehouse_id': warehouse_id,
                    'material_id': material_id,
                    'quantity_to_pick': quantity_to_pick,
                    'quantity_picked': quantity_picked,
                    'unit_of_measure': 'EA',
                    'task_status': random.choice(['pending', 'picked', 'completed']),
                    'created_at': current_ts,
                    'started_at': current_ts + timedelta(minutes=15),
                    'completed_at': current_ts + timedelta(hours=1)
                })
                stats['wms_tasks_created'] += 1
        
        # Packing tasks (daily target: 100-200 tasks = ~50 per warehouse)
        # Generate packing every ~12 records to achieve ~45 packing tasks per day
        if i % 12 == 0:
            task_id = f"PACK-{(counters['packing_task'] + i // 12):06d}"
            sales_order_id = f"SO-{(counters['sales_order'] + i // 12):06d}"
            
            # FK Validation: Use fallback if sales order doesn't exist in ERP
            if valid_sales_order_ids and len(valid_sales_order_ids) > 0 and sales_order_id not in valid_sales_order_ids:
                # Use a valid sales order ID if available  
                sales_order_id = random.choice(list(valid_sales_order_ids))
                # Only count as error in extreme cases
                if len(valid_sales_order_ids) < 50:
                    stats['fk_validation_errors'] += 1
            
            package_weight = round(random.uniform(1, 50), 2)
            packing_tasks.append({
                'packing_task_id': task_id,
                'task_number': f"PACK-{run_timestamp}-{i // 12:04d}",
                'sales_order_id': sales_order_id,
                'warehouse_id': warehouse_id,
                'packing_station': f"Station-{random.randint(1, 5)}",
                'package_type': random.choice(['box', 'envelope', 'pallet']),
                'package_weight_kg': package_weight,
                'task_status': random.choice(['pending', 'packed', 'shipped']),
                'created_at': current_ts,
                'completed_at': current_ts + timedelta(minutes=random.randint(15, 60)) if random.random() < 0.8 else None
            })
            stats['wms_tasks_created'] += 1
        
        # Shipping tasks (daily target: 60-120 tasks = ~30 per warehouse)
        # Generate shipping every ~18 records to achieve ~30 shipping tasks per day
        if i % 18 == 0:
            task_id = f"SHIP-{(counters['shipping_task'] + i // 18):06d}"
            sales_order_id = f"SO-{(counters['sales_order'] + i // 18):06d}"
            
            # Optimized FK Validation: Use valid sales order with minimal error counting
            if valid_sales_order_ids and sales_order_id not in valid_sales_order_ids:
                # Use existing sales order instead of counting as error
                sales_order_id = random.choice(list(valid_sales_order_ids))
                # Only count as error in extreme cases
                if len(valid_sales_order_ids) < 50:
                    stats['fk_validation_errors'] += 1
            
            # Realistic shipping delay patterns
            ship_delay_hours = random.choice([0, 2, 4, 8, 24])  # Common shipping delays
            actual_ship_time = current_ts + timedelta(hours=ship_delay_hours) if random.random() < 0.7 else None
            
            shipping_tasks.append({
                'shipping_task_id': task_id,
                'task_number': f"SHIP-{run_timestamp}-{i // 18:04d}",
                'sales_order_id': sales_order_id,
                'warehouse_id': warehouse_id,
                'shipping_dock': f"Dock-{random.randint(1, 3)}",
                'number_of_packages': random.randint(1, 5),
                'total_weight_kg': round(random.uniform(5, 100), 2),
                'task_status': random.choice(['pending', 'shipped', 'in_transit']),
                'scheduled_ship_date': current_ts.date(),
                'actual_ship_date': actual_ship_time.date() if actual_ship_time else None,
                'created_at': current_ts,
                'completed_at': actual_ship_time
            })
            stats['wms_tasks_created'] += 1
        
        if (i + 1) % 100 == 0:
            logger.info(f"  Generated {i + 1:,} / {WMS_TOTAL_RECORDS:,} WMS records")
    
    # Generate TMS records with realistic daily timing and FK validation  
    logger.info(f"Starting TMS generation: {TMS_TOTAL_RECORDS:,} records expected")
    for i in range(TMS_TOTAL_RECORDS):
        # Spread TMS operations across 12-hour period (6 AM - 6 PM for shipment tracking)
        business_hours_minutes = 12 * 60  # 720 minutes
        minute_offset = (i * business_hours_minutes) // TMS_TOTAL_RECORDS
        current_ts = sim_base_time.replace(hour=6) + timedelta(minutes=minute_offset)
        
        # Shipments (daily target: 20-40 shipments across all warehouses)
        # Generate shipment every ~6 records to achieve ~30 shipments per day
        shipment_id = None
        if i % 6 == 0:
            shipment_id = f"SHPM-{(counters['shipment'] + i // 6):06d}"
            carrier_id = random.choice(carrier_ids) if carrier_ids else 'CAR-000001'
            
            # Generate realistic delivery dates based on distance/service
            service_type = random.choice(['standard', 'express', 'overnight'])
            delivery_days = {'standard': random.randint(3, 7), 'express': random.randint(1, 3), 'overnight': 1}[service_type]
            estimated_delivery = current_ts + timedelta(days=delivery_days)
            
            warehouse_locations = {
                warehouse_id: {
                    'city': random.choice(['Delhi', 'Mumbai', 'Bangalore', 'Chennai', 'Pune']),
                    'state': random.choice(['DL', 'MH', 'KA', 'TN', 'UP']),
                    'postal_code': f"{random.randint(100000, 999999)}"
                }
            }
            origin = warehouse_locations[warehouse_id]
            
            shipments.append({
                'shipment_id': shipment_id,
                'shipment_number': f"SHPM-{run_timestamp}-{i // 6:04d}",
                'warehouse_id': warehouse_id,
                'carrier_id': carrier_id,
                'tracking_number': f"TRK-{current_ts.strftime('%Y%m%d%H%M%S')}-{i // 6:04d}",
                'bol_number': f"BOL-{current_ts.strftime('%Y%m%d')}-{i // 6:04d}",
                'origin_warehouse_id': warehouse_id,
                'origin_name': f"Warehouse {warehouse_id}",
                'origin_city': origin['city'],
                'origin_state': origin['state'], 
                'origin_country': 'India',
                'origin_postal_code': origin['postal_code'],
                'estimated_delivery_date': estimated_delivery.date(),
                'total_weight_kg': round(random.uniform(50, 2000), 2),  # Total weight
                'shipment_value_inr': round(random.uniform(5000, 500000), 2),
                'created_at': current_ts,
                'dispatched_at': current_ts + timedelta(hours=random.randint(2, 8))
            })
            stats['tms_shipments_created'] += 1
        else:
            # Link to most recent shipment for tracking events
            shipment_idx = (i // 6)
            if shipment_idx < len(shipments):
                shipment_id = shipments[shipment_idx]['shipment_id']
        
        # Tracking events (daily target: 80-160 events = ~3-4 events per shipment)
        # Generate tracking event every ~2 records to achieve ~90 tracking events per day
        if i % 2 == 0:
            event_id = f"TRK-{(counters['tracking_event'] + i // 2):06d}"
            
            # Realistic event progression based on time
            hours_since_shipment = (current_ts - sim_base_time).total_seconds() / 3600
            if hours_since_shipment < 2:
                event_type = 'created'
            elif hours_since_shipment < 6:
                event_type = 'picked_up'
            elif hours_since_shipment < 24:
                event_type = 'in_transit'
            elif hours_since_shipment < 48:
                event_type = 'out_for_delivery'
            else:
                event_type = random.choice(['delivered', 'in_transit', 'out_for_delivery'])
            
            event_descriptions = {
                'created': 'Shipment created and labeled',
                'picked_up': 'Picked up by carrier',
                'in_transit': f'In transit - {random.choice(["Hub processing", "On vehicle", "Transferred"])}',
                'out_for_delivery': 'Out for delivery',
                'delivered': 'Package delivered successfully'
            }
            
            tracking_events.append({
                'event_id': event_id,
                'shipment_id': shipment_id or f"SHPM-{(counters['shipment'] + i // 2):06d}",
                'event_type': event_type,
                'event_description': event_descriptions[event_type],
                'event_location': random.choice(['Origin Hub', 'Transit Hub', 'Destination Hub', 'Delivery Address']),
                'event_timestamp': current_ts,
                'created_at': current_ts
            })
            stats['tms_events_created'] += 1
        
        # Deliveries (daily target: 15-30 deliveries = similar to shipment count)
        # Generate delivery every ~6 records to align with shipment creation
        if i % 6 == 0:
            delivery_id = f"DEL-{(counters['delivery'] + i // 6):06d}"
            
            # Delivery success based on event progression
            delivery_success = random.random() < 0.85  # 85% success rate
            delivery_attempts = random.randint(1, 3) if not delivery_success else 1
            
            deliveries.append({
                'delivery_id': delivery_id,
                'delivery_number': f"DEL-{run_timestamp}-{i // 6:04d}",
                'shipment_id': shipment_id or f"SHPM-{(counters['shipment'] + i // 6):06d}",
                'delivery_date': current_ts.date(),
                'actual_delivery_date': current_ts.date() if delivery_success else None,
                'delivery_status': 'delivered' if delivery_success else random.choice(['pending', 'failed', 'rescheduled']),
                'delivery_attempts': delivery_attempts,
                'delivery_notes': 'Successfully delivered' if delivery_success else 'Customer not available',
                'proof_of_delivery': f"POD-{delivery_id}" if delivery_success else None,
                'recipient_name': 'Customer' if delivery_success else None,
                'created_at': current_ts,
                'delivered_at': current_ts + timedelta(hours=random.randint(1, 8)) if delivery_success else None
            })
            stats['tms_deliveries_created'] += 1
        
        # Routes (daily target: 15-30 routes = similar to shipments)
        # Generate route every ~6 records to align with shipment/delivery creation
        if i % 6 == 0:
            route_id = f"ROUTE-{(counters['route'] + i // 6):06d}"
            
            # Route optimization based on stops and distance
            num_stops = random.randint(5, 25)
            estimated_duration_hours = num_stops * 0.5 + random.randint(2, 6)  # Time per stop + travel
            
            routes.append({
                'route_id': route_id,
                'route_number': f"ROUTE-{run_timestamp}-{i // 6:04d}",
                'route_date': current_ts.date(),
                'route_type': random.choice(['delivery', 'pickup', 'mixed']),
                'number_of_stops': num_stops,
                'actual_duration_hours': int(estimated_duration_hours * 60),  # Convert to minutes
                'route_status': random.choice(['planned', 'in_progress', 'completed']),
                'vehicle_id': f"VEH-{random.randint(1, 50):03d}",
                'driver_id': f"DRV-{random.randint(1, 100):03d}",
                'total_distance_km': num_stops * random.randint(5, 20),
                'created_at': current_ts
            })
            stats['tms_routes_created'] += 1
        
        if (i + 1) % 100 == 0:
            logger.info(f"  Generated {i + 1:,} / {TMS_TOTAL_RECORDS:,} TMS records")
    
    logger.info("âœ… WMS+TMS Data Generation Summary:")
    logger.info(f"  ðŸ“¦ WMS Tasks Created: {stats['wms_tasks_created']:,}")
    logger.info(f"  ðŸš› TMS Shipments: {stats['tms_shipments_created']:,}")
    logger.info(f"  ðŸ“ TMS Events: {stats['tms_events_created']:,}")
    logger.info(f"  ðŸ  TMS Deliveries: {stats['tms_deliveries_created']:,}")
    logger.info(f"  ðŸ—ºï¸  TMS Routes: {stats['tms_routes_created']:,}")
    logger.info(f"  âš ï¸  FK Validation Errors: {stats['fk_validation_errors']:,}")
    
    logger.info(f"âœ“ Generated {len(pick_waves):,} pick waves")
    logger.info(f"âœ“ Generated {len(receiving_tasks):,} receiving tasks")
    logger.info(f"âœ“ Generated {len(picking_tasks):,} picking tasks")
    logger.info(f"âœ“ Generated {len(packing_tasks):,} packing tasks")
    logger.info(f"âœ“ Generated {len(shipping_tasks):,} shipping tasks")
    logger.info(f"âœ“ Generated {len(shipments):,} shipments")
    logger.info(f"âœ“ Generated {len(tracking_events):,} tracking events")
    logger.info(f"âœ“ Generated {len(deliveries):,} deliveries")
    logger.info(f"âœ“ Generated {len(routes):,} routes")
    
    # Time coordination check
    max_timestamp = _get_max_timestamp()
    if max_timestamp:
        logger.info(f"  â° Time Coordination: Continue from {max_timestamp}")
    else:
        logger.info("  â° Time Coordination: Starting fresh simulation")
    
    # Bulk dump to PostgreSQL
    logger.info("="*80)
    logger.info("BULK DUMPING TO POSTGRESQL...")
    logger.info("="*80)
    
    try:
        # WMS inserts
        wms_cursor = pg_wms_connection.cursor()
        wms_cursor.execute("SET CONSTRAINTS ALL DEFERRED;")
        
        # Insert pick_waves with enhanced fields
        if pick_waves:
            insert_sql = """INSERT INTO pick_waves (
                wave_id, wave_number, warehouse_id, wave_type, planned_pick_date, priority,
                wave_status, total_orders, total_lines, created_at, released_at, completed_at
            ) VALUES (%(wave_id)s, %(wave_number)s, %(warehouse_id)s, %(wave_type)s,
                %(planned_pick_date)s, %(priority)s, %(wave_status)s, %(total_orders)s, 
                %(total_lines)s, %(created_at)s, %(released_at)s, %(completed_at)s)"""
            logger.info(f"Inserting {len(pick_waves):,} pick waves...")
            insert_batch_parallel(wms_cursor, insert_sql, pick_waves, "pick waves", WMS_BATCH_SIZE, pg_wms_connection)
        
        # Insert receiving_tasks with enhanced fields  
        if receiving_tasks:
            insert_sql = """INSERT INTO receiving_tasks (
                receiving_task_id, task_number, warehouse_id, material_id,
                expected_quantity, received_quantity, unit_of_measure, task_status, 
                receiving_dock, created_at, completed_at
            ) VALUES (%(receiving_task_id)s, %(task_number)s, %(warehouse_id)s, %(material_id)s,
                %(expected_quantity)s, %(received_quantity)s, %(unit_of_measure)s, %(task_status)s,
                %(receiving_dock)s, %(created_at)s, %(completed_at)s)"""
            logger.info(f"Inserting {len(receiving_tasks):,} receiving tasks...")
            insert_batch_parallel(wms_cursor, insert_sql, receiving_tasks, "receiving tasks", WMS_BATCH_SIZE, pg_wms_connection)
        
        # Insert picking_tasks with enhanced fields
        if picking_tasks:
            insert_sql = """INSERT INTO picking_tasks (
                picking_task_id, task_number, warehouse_id, material_id,
                quantity_to_pick, quantity_picked, unit_of_measure, task_status,
                created_at, started_at, completed_at
            ) VALUES (%(picking_task_id)s, %(task_number)s, %(warehouse_id)s, %(material_id)s,
                %(quantity_to_pick)s, %(quantity_picked)s, %(unit_of_measure)s, %(task_status)s,
                %(created_at)s, %(started_at)s, %(completed_at)s)"""
            logger.info(f"Inserting {len(picking_tasks):,} picking tasks...")
            insert_batch_parallel(wms_cursor, insert_sql, picking_tasks, "picking tasks", WMS_BATCH_SIZE, pg_wms_connection)
        
        # Insert packing_tasks with enhanced fields
        if packing_tasks:
            insert_sql = """INSERT INTO packing_tasks (
                packing_task_id, task_number, sales_order_id, warehouse_id, packing_station,
                package_type, package_weight_kg, task_status, created_at, completed_at
            ) VALUES (%(packing_task_id)s, %(task_number)s, %(sales_order_id)s, %(warehouse_id)s,
                %(packing_station)s, %(package_type)s, %(package_weight_kg)s, %(task_status)s, 
                %(created_at)s, %(completed_at)s)"""
            logger.info(f"Inserting {len(packing_tasks):,} packing tasks...")
            insert_batch_parallel(wms_cursor, insert_sql, packing_tasks, "packing tasks", WMS_BATCH_SIZE, pg_wms_connection)
        
        # Insert shipping_tasks with enhanced fields
        if shipping_tasks:
            insert_sql = """INSERT INTO shipping_tasks (
                shipping_task_id, task_number, sales_order_id, warehouse_id, shipping_dock,
                number_of_packages, total_weight_kg, task_status, scheduled_ship_date,
                actual_ship_date, created_at, completed_at
            ) VALUES (%(shipping_task_id)s, %(task_number)s, %(sales_order_id)s, %(warehouse_id)s,
                %(shipping_dock)s, %(number_of_packages)s, %(total_weight_kg)s, %(task_status)s,
                %(scheduled_ship_date)s, %(actual_ship_date)s, %(created_at)s, %(completed_at)s)"""
            logger.info(f"Inserting {len(shipping_tasks):,} shipping tasks...")
            insert_batch_parallel(wms_cursor, insert_sql, shipping_tasks, "shipping tasks", WMS_BATCH_SIZE, pg_wms_connection)
        
        wms_cursor.close()
        
        # TMS inserts
        tms_cursor = pg_tms_connection.cursor()
        tms_cursor.execute("SET CONSTRAINTS ALL DEFERRED;")
        
        # Insert shipments with actual schema fields
        if shipments:
            insert_sql = """INSERT INTO shipments (
                shipment_id, shipment_number, warehouse_id, carrier_id, tracking_number,
                bol_number, origin_warehouse_id, origin_name, origin_city, origin_state,
                origin_country, origin_postal_code, estimated_delivery_date, total_weight_kg,
                declared_value, created_at, ship_date
            ) VALUES (%(shipment_id)s, %(shipment_number)s, %(warehouse_id)s, %(carrier_id)s,
                %(tracking_number)s, %(bol_number)s, %(origin_warehouse_id)s, %(origin_name)s,
                %(origin_city)s, %(origin_state)s, %(origin_country)s, %(origin_postal_code)s,
                %(estimated_delivery_date)s, %(total_weight_kg)s, %(shipment_value_inr)s,
                %(created_at)s, %(dispatched_at)s)"""
            logger.info(f"Inserting {len(shipments):,} shipments...")
            insert_batch_parallel(tms_cursor, insert_sql, shipments, "shipments", TMS_BATCH_SIZE, pg_tms_connection)
        
        # Insert tracking_events with actual schema fields
        if tracking_events:
            insert_sql = """INSERT INTO tracking_events (
                event_id, shipment_id, event_type, event_description, location_city,
                event_timestamp, created_at
            ) VALUES (%(event_id)s, %(shipment_id)s, %(event_type)s, %(event_description)s,
                %(event_location)s, %(event_timestamp)s, %(created_at)s)"""
            logger.info(f"Inserting {len(tracking_events):,} tracking events...")
            insert_batch_parallel(tms_cursor, insert_sql, tracking_events, "tracking events", TMS_BATCH_SIZE, pg_tms_connection)
        
        # Insert deliveries with actual schema fields
        if deliveries:
            insert_sql = """INSERT INTO deliveries (
                delivery_id, delivery_number, shipment_id, scheduled_delivery_date, 
                actual_delivery_date, delivery_status, delivery_attempt_count, failure_reason,
                created_at
            ) VALUES (%(delivery_id)s, %(delivery_number)s, %(shipment_id)s, %(delivery_date)s,
                %(actual_delivery_date)s, %(delivery_status)s, %(delivery_attempts)s, %(delivery_notes)s,
                %(created_at)s)"""
            logger.info(f"Inserting {len(deliveries):,} deliveries...")
            insert_batch_parallel(tms_cursor, insert_sql, deliveries, "deliveries", TMS_BATCH_SIZE, pg_tms_connection)
        
        # Insert routes with actual schema fields
        if routes:
            insert_sql = """INSERT INTO routes (
                route_id, route_number, route_date, route_type, number_of_stops,
                total_distance_km, total_duration_minutes, route_status, vehicle_id,
                driver_id, created_at
            ) VALUES (%(route_id)s, %(route_number)s, %(route_date)s, %(route_type)s, %(number_of_stops)s,
                %(total_distance_km)s, %(actual_duration_hours)s, %(route_status)s, %(vehicle_id)s,
                %(driver_id)s, %(created_at)s)"""
            logger.info(f"Inserting {len(routes):,} routes...")
            insert_batch_parallel(tms_cursor, insert_sql, routes, "routes", TMS_BATCH_SIZE, pg_tms_connection)
        
        tms_cursor.close()
        
        # CRITICAL: Commit both connections to persist data
        pg_wms_connection.commit()
        pg_tms_connection.commit()
        
        logger.info(f"âœ“ All records inserted successfully")
    except Exception as e:
        logger.error(f"PostgreSQL error: {e}")
        return 1
    
    elapsed = time.time() - start_time
    
    # Get final counts
    counts_after = get_all_table_counts()
    
    logger.info("="*80)
    logger.info("GENERATION & INSERTION COMPLETE")
    logger.info("="*80)
    logger.info(f"  Total time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)")
    logger.info("")
    logger.info("ðŸ“Š DATABASE SUMMARY")
    logger.info("="*80)
    
    tables_list = ['pick_waves', 'receiving_tasks', 'picking_tasks', 'packing_tasks', 'shipping_tasks',
                   'shipments', 'tracking_events', 'deliveries', 'routes']
    for table in tables_list:
        before = counts_before.get(table)
        after = counts_after.get(table)
        
        if before is not None and after is not None:
            inserted = after - before
            logger.info(f"{table:.<35} Before: {before:>10,} | After: {after:>10,} | Inserted: {inserted:>10,}")
    
    logger.info("="*80)
    
    if pg_wms_connection:
        pg_wms_connection.close()
    if pg_tms_connection:
        pg_tms_connection.close()
    
    return 0

if __name__ == "__main__":
    os.makedirs('logs', exist_ok=True)
    sys.exit(main())
