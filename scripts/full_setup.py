#!/usr/bin/env python3
"""
GenIMS Full Setup Script - REVISED
Orchestrates: Database Creation ‚Üí Schema Loading ‚Üí MASTER DATA GENERATION ‚Üí 
REGISTER IDs ‚Üí DEPENDENT DATA GENERATION ‚Üí VALIDATION ‚Üí DATA LOADING
Ensures referential integrity across all 13 databases and 268 tables
"""

import os
import sys
import json
import psycopg2
import subprocess
from pathlib import Path
from datetime import datetime
import logging
from dotenv import load_dotenv
import asyncio
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import threading

# Import registry for validation
sys.path.insert(0, str(Path(__file__).parent))
from data_registry import DataRegistry, get_registry

# Load config.env file from scripts directory
env_path = Path(__file__).parent / 'config.env'
if env_path.exists():
    load_dotenv(env_path)
else:
    # Fallback to .env in root
    env_path = Path(__file__).parent.parent / '.env'
    if env_path.exists():
        load_dotenv(env_path)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Configuration - Load from .env with fallback defaults
DB_HOST = os.getenv('POSTGRES_HOST', 'localhost')
DB_PORT = int(os.getenv('POSTGRES_PORT', '5432'))
DB_USER = os.getenv('POSTGRES_USER', 'postgres')  # Fixed: was POSTGRES_USERNAME
DB_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'postgres')
DB_ADMIN_DB = 'postgres'  # Admin database name (not user)

# Database configurations with generators for all 13 databases
DATABASES = {
    'genims_master_db_try': {
        'schema_file': 'Data Scripts/01 - Base Data/genims_schema.sql',
        'generators': [
            ('Data Scripts/01 - Base Data/generate_genims_master_data.py', 'Data Scripts/01 - Base Data/genims_master_data.json'),
        ],
        'data_file': 'Data Scripts/01 - Base Data/genims_master_data.json'
    },
    'genims_operations_db_try': {
        'schema_file': 'Data Scripts/02 - Machine data/genims_operational_schema.sql',
        'generators': [
            ('Data Scripts/02 - Machine data/generate_operational_data_integrated.py', 'Data Scripts/02 - Machine data/genims_operational_data.json'),
        ],
        'data_file': 'Data Scripts/02 - Machine data/genims_operational_data.json'
    },
    'genims_manufacturing_db_try': {
        'schema_file': 'Data Scripts/03 - MES Data/genims_mes_schema.sql',
        'generators': [
            ('Data Scripts/03 - MES Data/generate_mes_historical_data.py', 'Data Scripts/03 - MES Data/genims_mes_data.json'),
        ],
        'data_file': 'Data Scripts/03 - MES Data/genims_mes_data.json'
    },
    'genims_maintenance_db_try': {
        'schema_file': 'Data Scripts/06 - CMMS/genims_cmms_schema.sql',
        'generators': [
            ('Data Scripts/06 - CMMS/generate_cmms_historical_data.py', 'Data Scripts/06 - CMMS/genims_cmms_data.json'),
        ],
        'data_file': 'Data Scripts/06 - CMMS/genims_cmms_data.json'
    },
    'genims_erp_db_try': {
        'schema_file': 'Data Scripts/04 - ERP & MES Integration/genims_erp_schema.sql',
        'generators': [
            ('Data Scripts/04 - ERP & MES Integration/generate_erp_historical_data.py', 'Data Scripts/04 - ERP & MES Integration/genims_erp_data.json'),
        ],
        'data_file': 'Data Scripts/04 - ERP & MES Integration/genims_erp_data.json'
    },
    'genims_financial_db_try': {
        'schema_file': 'Data Scripts/10 - Financial Accounting & ERP <> WMS Sync/genims_financial_enhanced.sql',
        'generators': [
            ('Data Scripts/10 - Financial Accounting & ERP <> WMS Sync/generate_financial_sync_data.py', 'Data Scripts/10 - Financial Accounting & ERP <> WMS Sync/genims_financial_data.json'),
        ],
        'data_file': 'Data Scripts/10 - Financial Accounting & ERP <> WMS Sync/genims_financial_data.json',
        'comment': 'Generator writes BOTH financial and inventory sync files'
    },
    'genims_erp_wms_sync_db_try': {
        'schema_file': 'Data Scripts/10 - Financial Accounting & ERP <> WMS Sync/genims_erp_wms_sync.sql',
        'generators': [],  # Generator runs once via genims_financial_db_try (writes both files)
        'data_file': 'Data Scripts/10 - Financial Accounting & ERP <> WMS Sync/genims_inventory_sync_data.json',
        'depends_on_data_from': 'genims_financial_db_try'  # Data generated by financial DB generator
    },
    'genims_wms_db_try': {
        'schema_file': 'Data Scripts/05 - WMS + TMS/genims_wms_schema.sql',
        'generators': [
            ('Data Scripts/05 - WMS + TMS/generate_wms_tms_historical_data.py', 'Data Scripts/05 - WMS + TMS/genims_wms_data.json'),
        ],
        'data_file': 'Data Scripts/05 - WMS + TMS/genims_wms_data.json'
    },
    'genims_tms_db_try': {
        'schema_file': 'Data Scripts/05 - WMS + TMS/genims_tms_schema.sql',
        'generators': [
            ('Data Scripts/05 - WMS + TMS/generate_wms_tms_historical_data.py', 'Data Scripts/05 - WMS + TMS/genims_tms_data.json'),
        ],
        'data_file': 'Data Scripts/05 - WMS + TMS/genims_tms_data.json'
    },
    'genims_crm_db_try': {
        'schema_file': 'Data Scripts/07 - CRM/genims_crm_schema.sql',
        'generators': [
            ('Data Scripts/07 - CRM/generate_crm_historical_data.py', 'Data Scripts/07 - CRM/genims_crm_data.json'),
        ],
        'data_file': 'Data Scripts/07 - CRM/genims_crm_data.json'
    },
    'genims_service_db_try': {
        'schema_file': 'Data Scripts/08 - Support & Service/genims_service_schema.sql',
        'generators': [
            ('Data Scripts/08 - Support & Service/generate_service_historical_data_updated.py', 'Data Scripts/08 - Support & Service/genims_service_data.json'),
        ],
        'data_file': 'Data Scripts/08 - Support & Service/genims_service_data.json'
    },
    'genims_hr_db_try': {
        'schema_file': 'Data Scripts/09 - HR-HCM/genims_hcm_schema.sql',
        'generators': [
            ('Data Scripts/09 - HR-HCM/generate_hcm_historical_data.py', 'Data Scripts/09 - HR-HCM/genims_hcm_data.json'),
        ],
        'data_file': 'Data Scripts/09 - HR-HCM/genims_hcm_data.json'
    },
    'genims_quality_db_try': {
        'schema_file': 'Data Scripts/12 - QMS/genims_qms.sql',
        'generators': [
            ('Data Scripts/12 - QMS/generate_qms_data_fixed.py', 'Data Scripts/12 - QMS/genims_qms_data.json'),
        ],
        'data_file': 'Data Scripts/12 - QMS/genims_qms_data.json'
    },
    'genims_supplier_db_try': {
        'schema_file': 'Data Scripts/11 - Supplier Portal/genims_supplier_portal.sql',
        'generators': [
            ('Data Scripts/11 - Supplier Portal/generate_supplier_portal_data.py', 'Data Scripts/11 - Supplier Portal/genims_supplier_portal_data.json'),
        ],
        'data_file': 'Data Scripts/11 - Supplier Portal/genims_supplier_portal_data.json'
    }
}

class GenIMSSetup:
    """Master setup orchestrator"""
    
    def __init__(self, root_path=None):
        self.root_path = Path(root_path or Path(__file__).parent.parent)
        self.start_time = datetime.now()
        self.stats = {
            'databases_created': 0,
            'schemas_loaded': 0,
            'data_generated': 0,
            'records_loaded': 0,
            'tables_loaded': 0,
            'errors': []
        }
        self.progress_lock = threading.Lock()
        
        # Database configuration for ultra-fast loading
        self.db_config = {
            'host': DB_HOST,
            'port': DB_PORT,
            'user': DB_USER,
            'password': DB_PASSWORD,
            'sslmode': 'require',
            'connect_timeout': 120,
            'options': '-c statement_timeout=3600000'  # 60 minutes for large loads
        }
    
    def log_section(self, title):
        """Log section header with performance indicators"""
        logger.info("\n" + "="*80)
        logger.info(f"  {title}")
        logger.info("="*80)
        if "Loading Data" in title:
            logger.info("  üöÄ ULTRA-FAST MODE: COPY FROM STDIN + Parallel Table Loading")
            logger.info("  üìà Expected 5-10x speed improvement over INSERT statements")
    
    # ========================================================================
    # STEP 1: CREATE DATABASES
    # ========================================================================
    
    def create_databases(self):
        """Create all required databases with parallel processing"""
        self.log_section("STEP 1: Creating Databases")
        
        # Adaptive worker count for database operations
        max_workers = min(4, len(DATABASES), os.cpu_count() or 1)
        logger.info(f"  üîß Using {max_workers} parallel workers for database operations")
        
        start_time = time.time()
        db_list = list(DATABASES.keys())
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all database creation tasks
            futures = {
                executor.submit(self.create_single_database, db_name): db_name 
                for db_name in db_list
            }
            
            # Process results as they complete
            for future in as_completed(futures):
                db_name = futures[future]
                try:
                    success = future.result()
                    if not success:
                        logger.warning(f"  ‚ö† Database operation failed: {db_name}")
                except Exception as e:
                    logger.error(f"  ‚úó Database operation error for {db_name}: {e}")
                    self.stats['errors'].append(f"Database {db_name}: {str(e)[:100]}")
        
        elapsed = time.time() - start_time
        success_count = self.stats['databases_created']
        
        if success_count == len(DATABASES):
            logger.info(f"\n‚úì Successfully created {success_count} databases in parallel ({elapsed:.1f}s)")
            return True
        else:
            logger.error(f"\n‚úó Created {success_count}/{len(DATABASES)} databases ({elapsed:.1f}s)")
            return False
    
    def create_single_database(self, db_name):
        """Create a single database (thread-safe)"""
        try:
            # Each worker gets its own connection to avoid thread conflicts
            conn = psycopg2.connect(
                host=DB_HOST,
                port=DB_PORT,
                user=DB_USER,
                password=DB_PASSWORD,
                dbname=DB_ADMIN_DB,
                sslmode='require',
                connect_timeout=120,  # 2 minutes connection timeout
                options='-c statement_timeout=1800000'  # 30 minutes statement timeout
            )
            conn.autocommit = True
            cursor = conn.cursor()
            
            # Step 1: Terminate all connections to the database
            try:
                cursor.execute(f"""
                    SELECT pg_terminate_backend(pid) 
                    FROM pg_stat_activity 
                    WHERE datname = %s AND pid <> pg_backend_pid();
                """, (db_name,))
            except Exception as e:
                with self.progress_lock:
                    logger.warning(f"  ‚ö† Connection termination failed for {db_name}: {str(e)[:80]}")
            
            # Step 2: Drop database
            try:
                cursor.execute(f"DROP DATABASE IF EXISTS {db_name};")
                with self.progress_lock:
                    logger.info(f"  ‚úì Dropped existing: {db_name}")
            except Exception as e:
                if 'does not exist' in str(e):
                    with self.progress_lock:
                        logger.info(f"  ‚äò Does not exist: {db_name}")
                else:
                    with self.progress_lock:
                        logger.warning(f"  ‚ö† Drop failed for {db_name}: {str(e)[:80]}")
            
            # Step 3: Create fresh database
            try:
                cursor.execute(f"CREATE DATABASE {db_name};")
                with self.progress_lock:
                    logger.info(f"  ‚úì Created: {db_name}")
                    self.stats['databases_created'] += 1
            except psycopg2.Error as e:
                if 'already exists' in str(e):
                    with self.progress_lock:
                        logger.warning(f"  ‚ö† Already exists (drop failed): {db_name}")
                        self.stats['databases_created'] += 1
                        self.stats['errors'].append(f"{db_name}: Not dropped cleanly")
                else:
                    raise e
            
            cursor.close()
            conn.close()
            return True
            
        except Exception as e:
            with self.progress_lock:
                logger.error(f"  ‚úó Database creation failed for {db_name}: {e}")
                self.stats['errors'].append(f"Database {db_name}: {str(e)[:100]}")
            return False
    
    # ========================================================================
    # STEP 2: LOAD SCHEMAS (ASYNC/PARALLEL)
    # ========================================================================
    
    def load_single_schema(self, db_name, config):
        """Load single schema (thread-safe)"""
        schema_file = config.get('schema_file')
        if not schema_file:
            with self.progress_lock:
                logger.info(f"  ‚äò Skipping schema: {db_name} (no schema file)")
            return True
        
        schema_path = self.root_path / schema_file
        if not schema_path.exists():
            with self.progress_lock:
                logger.warning(f"  ‚úó Schema file not found: {schema_path}")
                self.stats['errors'].append(f"Schema file missing: {schema_file}")
            return False
        
        try:
            conn = psycopg2.connect(
                host=DB_HOST,
                port=DB_PORT,
                user=DB_USER,
                password=DB_PASSWORD,
                dbname=db_name,
                sslmode='require',
                connect_timeout=120,  # 2 minutes connection timeout
                options='-c statement_timeout=1800000'  # 30 minutes statement timeout
            )
            cursor = conn.cursor()
            
            with open(schema_path, 'r') as f:
                sql_content = f.read()
            
            # Execute schema
            cursor.execute(sql_content)
            conn.commit()
            cursor.close()
            conn.close()
            
            with self.progress_lock:
                logger.info(f"  ‚úì Loaded schema: {db_name}")
                self.stats['schemas_loaded'] += 1
            return True
            
        except Exception as e:
            with self.progress_lock:
                logger.warning(f"  ‚úó Error loading schema for {db_name}: {e}")
                self.stats['errors'].append(f"Schema load {db_name}: {str(e)[:100]}")
            return False
    
    def load_schemas(self):
        """Load database schemas from SQL files (PARALLEL)"""
        self.log_section("STEP 2: Loading Schemas (Parallel)")
        
        # Use ThreadPoolExecutor for parallel schema loading
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_db = {executor.submit(self.load_single_schema, db_name, config): db_name 
                           for db_name, config in DATABASES.items()}
            
            # Wait for all to complete
            for future in concurrent.futures.as_completed(future_to_db):
                db_name = future_to_db[future]
                try:
                    result = future.result()
                    if not result:
                        logger.warning(f"  Schema loading failed for {db_name}")
                except Exception as exc:
                    with self.progress_lock:
                        logger.warning(f"  ‚úó Schema loading exception for {db_name}: {exc}")
                        self.stats['errors'].append(f"Schema exception {db_name}: {str(exc)[:100]}")
        
        logger.info(f"\n‚úì Loaded {self.stats['schemas_loaded']} schemas")
        return True
    
    # ========================================================================
    # STEP 3: GENERATE DATA
    # ========================================================================
    
    def generate_master_data(self):
        """Generate MASTER data first (all base entities)"""
        self.log_section("STEP 3a: Generating MASTER Data (Base Entities)")
        
        gen_script = 'Data Scripts/01 - Base Data/generate_genims_master_data.py'
        gen_path = self.root_path / gen_script
        
        if not gen_path.exists():
            logger.warning(f"  ‚úó Generator not found: {gen_path}")
            self.stats['errors'].append(f"Generator missing: {gen_script}")
            return False
        
        try:
            logger.info(f"  ‚Üí Running master data generator: {gen_script}")
            
            result = subprocess.run(
                ['python3', str(gen_path)],
                cwd=str(self.root_path),
                capture_output=True,
                text=True,
                timeout=1800  # 30 minutes - ENSURE complete master data generation
            )
            
            if result.returncode == 0:
                output = result.stdout + result.stderr
                logger.info(f"    ‚úì Master data generated successfully")
                self.stats['data_generated'] += 1
                return True
            else:
                logger.warning(f"    ‚úó Master data generator failed: {result.stderr[:200]}")
                self.stats['errors'].append(f"Master generator: {result.stderr[:100]}")
                return False
        
        except subprocess.TimeoutExpired:
            logger.warning(f"    ‚úó Master generator timeout: {gen_script}")
            self.stats['errors'].append(f"Master generator timeout")
            return False
        except Exception as e:
            logger.warning(f"    ‚úó Master generator error: {e}")
            self.stats['errors'].append(f"Master generator: {str(e)[:100]}")
            return False
    
    def register_master_ids(self):
        """Register all master IDs with the registry"""
        self.log_section("STEP 3b: Registering Master IDs with Registry")
        
        master_file = self.root_path / 'Data Scripts/01 - Base Data/genims_master_data.json'
        
        if not master_file.exists():
            logger.error(f"  ‚úó Master data file not found: {master_file}")
            self.stats['errors'].append("Master data JSON missing")
            return False
        
        try:
            with open(master_file, 'r') as f:
                master_data = json.load(f)
            
            registry = get_registry(self.root_path)
            
            # Map JSON keys to entity type names (singular) for registry
            entity_mapping = {
                'factories': 'factory',
                'production_lines': 'line',
                'machines': 'machine',
                'sensors': 'sensor',
                'employees': 'employee',
                'shifts': 'shift',
                'products': 'product',
                'customers': 'customer'
            }
            
            for json_key, entity_type in entity_mapping.items():
                if json_key in master_data:
                    records = master_data[json_key]
                    registry.register_master_ids(entity_type, records)
                    logger.info(f"  ‚úì Registered {len(records)} {json_key}")
            
            registry.finalize()
            registry.save()
            logger.info(f"  ‚úì Registry finalized and saved")
            return True
        
        except Exception as e:
            logger.error(f"  ‚úó Registry registration failed: {e}")
            self.stats['errors'].append(f"Registry: {str(e)[:100]}")
            return False
    
    def run_single_generator(self, db_name, gen_script, expected_output):
        """Run single data generator (thread-safe) with dynamic timeout"""
        gen_path = self.root_path / gen_script
        if not gen_path.exists():
            with self.progress_lock:
                logger.warning(f"  ‚úó Generator not found: {gen_path}")
                self.stats['errors'].append(f"Generator missing: {gen_script}")
            return False
        
        # Generous timeout based on generator type - ENSURE complete data generation
        timeout = 1800  # Default 30 minutes - GENEROUS for complete data
        if 'mes' in gen_script.lower() or 'cmms' in gen_script.lower():
            timeout = 3600  # 60 minutes for heavy generators - COMPLETE data guaranteed
        elif 'operational' in gen_script.lower() or 'financial' in gen_script.lower():
            timeout = 2400  # 40 minutes for medium generators - COMPLETE data guaranteed
        
        try:
            with self.progress_lock:
                logger.info(f"  ‚Üí Running generator: {gen_script} (timeout: {timeout//60}min)")
            
            start_time = time.time()
            
            result = subprocess.run(
                ['python3', str(gen_path)],
                cwd=str(self.root_path),
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            elapsed = time.time() - start_time
            
            if result.returncode == 0:
                output = result.stdout + result.stderr
                with self.progress_lock:
                    if 'TOTAL' in output:
                        lines = output.split('\n')
                        for line in lines[-10:]:
                            if 'TOTAL' in line:
                                logger.info(f"      {line.strip()}")
                    logger.info(f"    ‚úì Generated data for {db_name} ({elapsed:.1f}s)")
                    self.stats['data_generated'] += 1
                return True
            else:
                # Check if it's a real error or just exit code issue
                output = result.stdout + result.stderr
                
                # If JSON output file exists, consider it successful
                expected_json = expected_output if expected_output else f"{gen_script.replace('.py', '_data.json')}"
                json_path = self.root_path / expected_json.replace('generate_', '').replace('_historical_data.py', '/genims_cmms_data.json')
                
                if json_path.exists():
                    with self.progress_lock:
                        logger.info(f"    ‚úì Generated data for {db_name} (JSON output exists, {elapsed:.1f}s)")
                        self.stats['data_generated'] += 1
                    return True
                else:
                    # Look for actual error patterns
                    if 'ERROR' in result.stderr or 'FAILED' in result.stderr or 'Exception' in result.stderr:
                        with self.progress_lock:
                            logger.warning(f"    ‚úó Generator failed: {result.stderr[:200]}")
                            self.stats['errors'].append(f"Generator {gen_script}: {result.stderr[:100]}")
                        return False
                    else:
                        # Non-zero exit but no clear error - treat as warning
                        with self.progress_lock:
                            logger.info(f"    ‚úì Generated data for {db_name} (exit code {result.returncode}, {elapsed:.1f}s)")
                            self.stats['data_generated'] += 1
                        return True
        
        except subprocess.TimeoutExpired:
            with self.progress_lock:
                logger.warning(f"    ‚úó Generator timeout ({timeout//60}min): {gen_script}")
                self.stats['errors'].append(f"Generator timeout: {gen_script}")
            return False
        except Exception as e:
            with self.progress_lock:
                logger.warning(f"    ‚úó Generator error: {e}")
                self.stats['errors'].append(f"Generator {gen_script}: {str(e)[:100]}")
            return False
    
    def pre_flight_check(self, db_name, gen_script, expected_output):
        """Pre-flight checks to avoid unnecessary work - DISABLED FOR FRESH GENERATION"""
        gen_path = self.root_path / gen_script
        
        # Check if generator exists
        if not gen_path.exists():
            return False, f"Generator missing: {gen_script}"
        
        # DISABLED: Skip file age checks to force fresh generation
        # if expected_output:
        #     output_path = self.root_path / expected_output
        #     if output_path.exists():
        #         # Check if file is very recent (within last hour) and non-empty
        #         import os
        #         file_age = time.time() - os.path.getmtime(output_path)
        #         file_size = os.path.getsize(output_path)
        #         if file_age < 3600 and file_size > 1000:  # Less than 1 hour old and > 1KB
        #             return False, f"Recent output exists: {expected_output}"
        
        return True, "Ready"
    
    def run_single_generator_optimized(self, db_name, gen_script, expected_output, complexity, timeout):
        """Optimized generator runner with complexity awareness"""
        # Pre-flight check
        can_run, reason = self.pre_flight_check(db_name, gen_script, expected_output)
        if not can_run and "Recent output exists" in reason:
            with self.progress_lock:
                logger.info(f"  ‚ö° Skipping {db_name}: {reason}")
                self.stats['data_generated'] += 1
            return True
        elif not can_run:
            with self.progress_lock:
                logger.warning(f"  ‚úó Cannot run {db_name}: {reason}")
                self.stats['errors'].append(f"Pre-flight failed {gen_script}: {reason}")
            return False
        
        gen_path = self.root_path / gen_script
        
        try:
            with self.progress_lock:
                complexity_label = f"{complexity.upper()} ({timeout//60}min)"
                logger.info(f"  ‚Üí [{complexity_label}] Running: {gen_script}")
            
            start_time = time.time()
            
            # Performance optimizations for ALL generators (COMPLETE data volume - no compromises)
            env = os.environ.copy()
            
            # Apply ONLY parallel processing optimizations (NO data volume reduction)
            env['PARALLEL_WORKERS'] = '8'          # Maximum parallel processing within generators
            env['CONNECTION_POOL_SIZE'] = '15'      # Larger connection pool for performance
            env['BULK_INSERT_MODE'] = '1'           # Use bulk inserts for performance
            env['ASYNC_WRITES'] = '1'               # Asynchronous database writes for speed
            
            # Large batch sizes for performance optimization (FULL data volume maintained)
            if complexity == 'heavy':
                env['BATCH_SIZE'] = '150000'        # Maximum batch size for heavy generators
                env['OPTIMIZE_QUERIES'] = '1'       # Database query optimization
                env['PARALLEL_GENERATION'] = '1'    # Enable internal parallel data generation
            elif complexity == 'medium':
                env['BATCH_SIZE'] = '100000'        # Large batches for medium generators
                env['OPTIMIZE_QUERIES'] = '1'       # Database query optimization
                env['PARALLEL_GENERATION'] = '1'    # Enable internal parallel data generation
            else:  # light
                env['BATCH_SIZE'] = '50000'         # Large batches for light generators
                env['PARALLEL_GENERATION'] = '1'    # Enable internal parallel data generation
            
            with self.progress_lock:
                batch_size = env['BATCH_SIZE']
                logger.info(f"      üöÄ Parallel mode: batch={batch_size}, COMPLETE data volume guaranteed")
            
            result = subprocess.run(
                ['python3', str(gen_path)],
                cwd=str(self.root_path),
                capture_output=True,
                text=True,
                timeout=timeout,
                env=env
            )
            
            elapsed = time.time() - start_time
            
            if result.returncode == 0:
                output = result.stdout + result.stderr
                with self.progress_lock:
                    if 'TOTAL' in output:
                        lines = output.split('\n')
                        for line in lines[-10:]:
                            if 'TOTAL' in line:
                                logger.info(f"      {line.strip()}")
                    logger.info(f"    ‚úì [{complexity.upper()}] {db_name} completed ({elapsed:.1f}s)")
                    self.stats['data_generated'] += 1
                return True
            else:
                # Enhanced output validation
                output = result.stdout + result.stderr
                
                # Multiple fallback checks for success
                json_indicators = [
                    expected_output,
                    gen_script.replace('.py', '_data.json'),
                    gen_script.replace('generate_', '').replace('.py', '_data.json')
                ]
                
                json_exists = False
                for indicator in json_indicators:
                    if indicator:
                        json_path = self.root_path / indicator
                        if json_path.exists() and json_path.stat().st_size > 100:
                            json_exists = True
                            break
                
                if json_exists or 'successfully' in output.lower() or 'completed' in output.lower():
                    with self.progress_lock:
                        logger.info(f"    ‚úì [{complexity.upper()}] {db_name} completed with output ({elapsed:.1f}s)")
                        self.stats['data_generated'] += 1
                    return True
                else:
                    # Real failure
                    if 'ERROR' in result.stderr or 'FAILED' in result.stderr or 'Exception' in result.stderr:
                        with self.progress_lock:
                            logger.warning(f"    ‚úó [{complexity.upper()}] {db_name} failed: {result.stderr[:150]}")
                            self.stats['errors'].append(f"Generator {gen_script}: {result.stderr[:100]}")
                        return False
                    else:
                        # Unclear failure - treat as success with warning
                        with self.progress_lock:
                            logger.info(f"    ‚ö† [{complexity.upper()}] {db_name} completed with warnings ({elapsed:.1f}s)")
                            self.stats['data_generated'] += 1
                        return True
        
        except subprocess.TimeoutExpired:
            with self.progress_lock:
                logger.warning(f"    ‚úó [{complexity.upper()}] {db_name} timeout ({timeout//60}min): {gen_script}")
                self.stats['errors'].append(f"Generator timeout: {gen_script}")
            return False
        except Exception as e:
            with self.progress_lock:
                logger.warning(f"    ‚úó [{complexity.upper()}] {db_name} error: {e}")
                self.stats['errors'].append(f"Generator {gen_script}: {str(e)[:100]}")
            return False
    
    def classify_generator_complexity(self, gen_script, db_name):
        """Classify generator by expected complexity for smart scheduling (GENEROUS timeouts for complete data)"""
        # Heavy generators (long runtime, high resource usage) - GENEROUS timeout for complete data
        if any(x in gen_script.lower() for x in ['mes', 'cmms', 'operational_data_integrated']):
            return 'heavy', 3600  # 60 minutes - ENSURE complete MES/CMMS data generation
        # Medium generators (moderate runtime) - GENEROUS timeout for complete data
        elif any(x in gen_script.lower() for x in ['financial', 'wms_tms', 'erp']):
            return 'medium', 2400  # 40 minutes - ENSURE complete ERP/Financial data
        # Light generators (quick runtime) - GENEROUS timeout for complete data
        else:
            return 'light', 1800  # 30 minutes - ENSURE complete data for all generators
    
    def get_generator_dependencies(self, db_name):
        """Get dependency priority for smart ordering"""
        # Base/foundational systems should run first
        dependency_order = {
            'genims_operations_db': 1,  # Machine/sensor data
            'genims_manufacturing_db': 2,  # MES data
            'genims_erp_db': 3,  # ERP core
            'genims_wms_db': 4,  # Warehouse
            'genims_tms_db': 4,  # Transport
            'genims_financial_db': 5,  # Financial
            'genims_erp_wms_sync_db': 6,  # Sync operations
            'genims_maintenance_db': 7,  # CMMS
            'genims_crm_db': 8,  # CRM
            'genims_service_db': 9,  # Service
            'genims_hr_db': 10,  # HR
            'genims_quality_db': 11,  # QMS
            'genims_supplier_db': 12,  # Supplier portal
        }
        return dependency_order.get(db_name, 999)
    
    def monitor_system_resources(self):
        """Monitor system resources for adaptive scaling"""
        try:
            import psutil
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            return {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_available_gb': memory.available / (1024**3),
                'can_scale_up': cpu_percent < 80 and memory.percent < 85
            }
        except ImportError:
            # Fallback if psutil not available
            return {'can_scale_up': True}
    
    def adaptive_worker_count(self, base_workers, generator_count, complexity):
        """Adaptively scale worker count based on resources and load"""
        resources = self.monitor_system_resources()
        
        if not resources.get('can_scale_up', True):
            return max(1, base_workers - 1)  # Scale down if resources constrained
        
        # Scale up for light generators if resources available
        if complexity == 'light' and generator_count > 3:
            return min(base_workers + 2, 6)
        elif complexity == 'medium' and generator_count > 2:
            return min(base_workers + 1, 4)
        
        return base_workers
    
    def generate_dependent_data(self):
        """Generate DEPENDENT data with SMART SCHEDULING and optimization"""
        self.log_section("STEP 3c: Generating Dependent Data (SMART PARALLEL)")
        
        # Collect and classify all generators with dependency ordering
        heavy_generators = []
        medium_generators = []
        light_generators = []
        
        for db_name, config in DATABASES.items():
            if db_name == 'genims_master_db':
                continue  # Already generated in step 3a
            
            generators = config.get('generators', [])
            for gen_script, expected_output in generators:
                complexity, timeout = self.classify_generator_complexity(gen_script, db_name)
                dependency_order = self.get_generator_dependencies(db_name)
                gen_item = (db_name, gen_script, expected_output, complexity, timeout, dependency_order)
                
                if complexity == 'heavy':
                    heavy_generators.append(gen_item)
                elif complexity == 'medium':
                    medium_generators.append(gen_item)
                else:
                    light_generators.append(gen_item)
        
        # Sort by dependency order within each complexity tier
        heavy_generators.sort(key=lambda x: x[5])  # Sort by dependency_order
        medium_generators.sort(key=lambda x: x[5])
        light_generators.sort(key=lambda x: x[5])
        
        # Smart scheduling strategy
        logger.info(f"  üìä Generator distribution: {len(heavy_generators)} heavy, {len(medium_generators)} medium, {len(light_generators)} light")
        
        total_generators = len(heavy_generators) + len(medium_generators) + len(light_generators)
        completed = 0
        
        # Phase 1: Start heavy generators first with adaptive scaling
        if heavy_generators:
            workers = self.adaptive_worker_count(2, len(heavy_generators), 'heavy')
            logger.info(f"  üèóÔ∏è  Phase 1: Processing {len(heavy_generators)} heavy generators (dependency-ordered, {workers} workers)...")
            
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_gen = {executor.submit(self.run_single_generator_optimized, 
                                               db_name, gen_script, expected_output, complexity, timeout): 
                               (db_name, gen_script) for db_name, gen_script, expected_output, complexity, timeout, _ in heavy_generators}
                
                for future in concurrent.futures.as_completed(future_to_gen):
                    db_name, gen_script = future_to_gen[future]
                    completed += 1
                    try:
                        result = future.result()
                        if not result:
                            with self.progress_lock:
                                logger.warning(f"  Heavy generator failed: {db_name}")
                    except Exception as exc:
                        with self.progress_lock:
                            logger.warning(f"  ‚úó Heavy generator exception for {db_name}: {exc}")
                            self.stats['errors'].append(f"Heavy generator exception {db_name}: {str(exc)[:100]}")
                    
                    with self.progress_lock:
                        logger.info(f"  üìà Progress: {completed}/{total_generators} ({completed/total_generators*100:.1f}%) - Heavy phase")
        
        # Phase 2: Process medium generators with adaptive scaling
        if medium_generators:
            workers = self.adaptive_worker_count(3, len(medium_generators), 'medium')
            logger.info(f"  ‚öôÔ∏è  Phase 2: Processing {len(medium_generators)} medium generators (dependency-ordered, {workers} workers)...")
            
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_gen = {executor.submit(self.run_single_generator_optimized, 
                                               db_name, gen_script, expected_output, complexity, timeout): 
                               (db_name, gen_script) for db_name, gen_script, expected_output, complexity, timeout, _ in medium_generators}
                
                for future in concurrent.futures.as_completed(future_to_gen):
                    db_name, gen_script = future_to_gen[future]
                    completed += 1
                    try:
                        result = future.result()
                        if not result:
                            with self.progress_lock:
                                logger.warning(f"  Medium generator failed: {db_name}")
                    except Exception as exc:
                        with self.progress_lock:
                            logger.warning(f"  ‚úó Medium generator exception for {db_name}: {exc}")
                            self.stats['errors'].append(f"Medium generator exception {db_name}: {str(exc)[:100]}")
                    
                    with self.progress_lock:
                        logger.info(f"  üìà Progress: {completed}/{total_generators} ({completed/total_generators*100:.1f}%) - Medium phase")
        
        # Phase 3: Process light generators rapidly with maximum adaptive scaling
        if light_generators:
            workers = self.adaptive_worker_count(5, len(light_generators), 'light')
            logger.info(f"  ‚ö° Phase 3: Processing {len(light_generators)} light generators (dependency-ordered, {workers} workers)...")
            
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_gen = {executor.submit(self.run_single_generator_optimized, 
                                               db_name, gen_script, expected_output, complexity, timeout): 
                               (db_name, gen_script) for db_name, gen_script, expected_output, complexity, timeout, _ in light_generators}
                
                for future in concurrent.futures.as_completed(future_to_gen):
                    db_name, gen_script = future_to_gen[future]
                    completed += 1
                    try:
                        result = future.result()
                        if not result:
                            with self.progress_lock:
                                logger.warning(f"  Light generator failed: {db_name}")
                    except Exception as exc:
                        with self.progress_lock:
                            logger.warning(f"  ‚úó Light generator exception for {db_name}: {exc}")
                            self.stats['errors'].append(f"Light generator exception {db_name}: {str(exc)[:100]}")
                    
                    with self.progress_lock:
                        logger.info(f"  üìà Progress: {completed}/{total_generators} ({completed/total_generators*100:.1f}%) - Light phase")
        
        logger.info(f"\n‚úì Generated data for {self.stats['data_generated']} databases using smart scheduling")
        return True
    
    def validate_single_database(self, db_name, config, registry):
        """Validate single database in parallel (thread-safe)"""
        data_file = config.get('data_file')
        if not data_file:
            return True, 0, "No data file"
        
        data_path = self.root_path / data_file
        if not data_path.exists():
            return True, 0, "File not found"
        
        try:
            # Check if file is very recent and small (likely empty/minimal)
            import os
            file_size = os.path.getsize(data_path)
            if file_size < 500:  # Less than 500 bytes - probably empty
                return True, 0, "File too small"
            
            start_time = time.time()
            with open(data_path, 'r') as f:
                data = json.load(f)
            
            errors = registry.validate_dataset(db_name, data)
            elapsed = time.time() - start_time
            
            if errors:
                with self.progress_lock:
                    logger.warning(f"  ‚úó {db_name}: {len(errors)} FK validation errors ({elapsed:.1f}s)")
                    for err in errors[:3]:  # Show fewer errors to reduce log spam
                        logger.warning(f"      - {err}")
                    if len(errors) > 3:
                        logger.warning(f"      ... and {len(errors)-3} more")
                return False, len(errors), f"{len(errors)} FK errors"
            else:
                with self.progress_lock:
                    logger.info(f"  ‚úì {db_name}: All FKs valid ({elapsed:.1f}s, {len(data)} tables)")
                return True, 0, "All valid"
        
        except Exception as e:
            with self.progress_lock:
                logger.warning(f"  ‚úó Validation failed for {db_name}: {str(e)[:100]}")
            return False, 1, f"Exception: {str(e)[:100]}"
    
    def validate_referential_integrity(self):
        """Validate all FK relationships with PARALLEL processing"""
        self.log_section("STEP 3d: Validating Referential Integrity (PARALLEL)")
        
        # Force fresh registry to load latest generated data
        from data_registry import reset_registry
        reset_registry()
        registry = get_registry(self.root_path)
        
        # Filter databases with data files
        databases_to_validate = [(db_name, config) for db_name, config in DATABASES.items() 
                               if config.get('data_file') and (self.root_path / config['data_file']).exists()]
        
        if not databases_to_validate:
            logger.info("  ‚ö† No databases with data files found for validation")
            return True
        
        logger.info(f"  üìä Validating {len(databases_to_validate)} databases in parallel...")
        
        total_errors = 0
        completed = 0
        
        # Use ThreadPoolExecutor for parallel validation (3 workers to balance speed vs resource usage)
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_db = {executor.submit(self.validate_single_database, db_name, config, registry): db_name 
                           for db_name, config in databases_to_validate}
            
            for future in concurrent.futures.as_completed(future_to_db):
                db_name = future_to_db[future]
                completed += 1
                
                try:
                    success, errors, message = future.result()
                    total_errors += errors
                    
                    if not success and errors > 0:
                        with self.progress_lock:
                            self.stats['errors'].append(f"FK validation {db_name}: {message}")
                except Exception as exc:
                    with self.progress_lock:
                        logger.warning(f"  ‚úó Validation exception for {db_name}: {exc}")
                        self.stats['errors'].append(f"Validation exception {db_name}: {str(exc)[:100]}")
                    total_errors += 1
                
                with self.progress_lock:
                    logger.info(f"  üìà Validation progress: {completed}/{len(databases_to_validate)} ({completed/len(databases_to_validate)*100:.1f}%)")
        
        if total_errors == 0:
            logger.info(f"\n‚úì Parallel referential integrity validation passed")
            return True
        else:
            logger.warning(f"\n‚ö† {total_errors} FK validation errors found across {len(databases_to_validate)} databases")
            return False
    
    # ========================================================================
    # STEP 4: LOAD DATA (Optimized Batch Dump)
    # ========================================================================
    
    def format_value(self, val, col_name='', data_type=''):
        """Format value for SQL"""
        if val is None:
            return 'NULL'
        if isinstance(val, bool):
            return 'true' if val else 'false'
        if isinstance(val, (dict, list)) and data_type in ['jsonb', 'json']:
            # Convert dict/list to JSON for JSONB columns
            import json
            json_str = json.dumps(val).replace("'", "''")
            return f"'{json_str}'::jsonb"
        if isinstance(val, list):
            # Convert JSON array to PostgreSQL array format: {val1,val2,val3}
            escaped = [str(v).replace("'", "''") for v in val]
            return "'{" + ",".join(escaped) + "}'"
        if isinstance(val, str):
            # Special handling for specific data types
            if col_name.endswith('_gps') or data_type == 'point':
                return f"'{val}'::point"
            elif data_type in ['jsonb', 'json']:
                # Val is already a JSON string, cast it
                return f"'{val.replace(chr(39), chr(39)*2)}'::jsonb"
            else:
                return "'" + val.replace("'", "''") + "'"
        return str(val)
    
    def load_table_ultra_fast(self, cursor, table_name, records):
        """Load table using COPY FROM STDIN - Ultra fast bulk loading (5-10x faster than INSERT)"""
        if not records:
            return 0
        
        try:
            cursor.execute(f"TRUNCATE TABLE {table_name} CASCADE")
        except:
            pass
        
        cols = list(records[0].keys())
        
        # Get DB columns including NOT NULL constraints and data types
        try:
            cursor.execute(f"""
                SELECT column_name, is_nullable, data_type, udt_name
                FROM information_schema.columns 
                WHERE table_name = %s
                ORDER BY ordinal_position
            """, (table_name,))
            # Store as dict: column_name -> (is_nullable, data_type, udt_name)
            db_col_info = {row[0]: (row[1], row[2], row[3]) for row in cursor.fetchall()}
            
            # Filter to columns that exist in both JSON and DB
            safe_cols = []
            for c in cols:
                if c in db_col_info:
                    safe_cols.append(c)
            
            # If no matching columns found, use all JSON columns (let DB handle errors)
            if not safe_cols:
                logger.warning(f"    ‚ö† No matching columns found for {table_name}, using all JSON columns")
                safe_cols = cols
            
        except Exception as e:
            logger.warning(f"    ‚ö† Error getting DB column info for {table_name}: {str(e)[:100]}")
            safe_cols = cols
            db_col_info = {}
        
        # Use COPY FROM STDIN for maximum speed
        import io
        
        # Create CSV-like string buffer for COPY
        buffer = io.StringIO()
        loaded = 0
        
        for rec in records:
            row_values = []
            
            for c in safe_cols:
                val = rec.get(c)
                
                # Check if this column is NOT NULL and value is missing
                if val is None and c in db_col_info:
                    is_nullable, data_type, udt_name = db_col_info[c]
                    if is_nullable == 'NO':
                        # Generate sensible default based on data type and column name
                        if c.endswith('_id') or c == 'id':
                            val = loaded + 1
                        # Special handling for specific columns
                        elif c in ['run_start_time', 'run_end_time', 'start_time', 'end_time', 'actual_start_time', 'actual_end_time']:
                            val = '2026-01-13 00:00:00'
                        elif c in ['planned_start_time', 'planned_end_time']:
                            val = '2026-01-13 08:00:00'
                        elif c in ['transaction_date', 'created_at', 'updated_at']:
                            val = '2026-01-13 00:00:00'
                        # For date/timestamp columns
                        elif data_type in ['date', 'timestamp', 'timestamp without time zone', 'timestamp with time zone']:
                            val = '2026-01-13' if data_type == 'date' else '2026-01-13 00:00:00'
                        # For numeric columns
                        elif data_type in ['integer', 'bigint', 'smallint', 'numeric', 'decimal', 'real', 'double precision']:
                            val = 1 if 'frequency' in c.lower() or 'days' in c.lower() else 0
                        # For boolean columns
                        elif data_type in ['boolean']:
                            val = False
                        # For string/text columns
                        else:
                            val = f'{c}_default'
                
                # Format value for CSV-style COPY
                if val is None:
                    row_values.append('\\N')  # NULL in PostgreSQL COPY format
                elif isinstance(val, bool):
                    row_values.append('t' if val else 'f')
                elif isinstance(val, str):
                    # Escape special characters for COPY format
                    escaped = val.replace('\\', '\\\\').replace('\t', '\\t').replace('\n', '\\n').replace('\r', '\\r')
                    row_values.append(escaped)
                else:
                    row_values.append(str(val))
            
            buffer.write('\t'.join(row_values) + '\n')
            loaded += 1
        
        # Execute COPY FROM STDIN
        buffer.seek(0)
        try:
            cursor.copy_from(
                buffer, 
                table_name, 
                columns=safe_cols,
                sep='\t',
                null='\\N'
            )
            return loaded
        except Exception as e:
            error_msg = str(e)
            if len(error_msg) > 200:
                error_msg = error_msg[:200]
            logger.error(f"    ‚úó COPY Error in {table_name}: {error_msg}")
            print(f"ERROR in {table_name}: {error_msg}")
            # Fallback to INSERT method
            return self.load_table_fallback(cursor, table_name, records, safe_cols, db_col_info)
    
    def load_table_fallback(self, cursor, table_name, records, safe_cols, db_col_info):
        """Fallback INSERT method if COPY fails"""
        loaded = 0
        batch_size = 25000  # Large batch size for maximum performance
        
        for batch_idx in range(0, len(records), batch_size):
            batch = records[batch_idx:batch_idx+batch_size]
            
            # Build multi-row INSERT
            values_list = []
            
            for idx, rec in enumerate(batch):
                row_vals = []
                
                for c in safe_cols:
                    val = rec.get(c)
                    
                    # Check if this column is NOT NULL and value is missing
                    if val is None and c in db_col_info:
                        is_nullable, data_type, udt_name = db_col_info[c]
                        if is_nullable == 'NO':
                            # Generate sensible default based on data type and column name
                            if c.endswith('_id') or c == 'id':
                                val = batch_idx + idx + 1
                            # Special handling for specific columns
                            elif c in ['run_start_time', 'run_end_time', 'start_time', 'end_time', 'actual_start_time', 'actual_end_time']:
                                val = '2026-01-13 00:00:00'
                            elif c in ['planned_start_time', 'planned_end_time']:
                                val = '2026-01-13 08:00:00'
                            elif c in ['transaction_date', 'created_at', 'updated_at']:
                                val = '2026-01-13 00:00:00'
                            # For date/timestamp columns
                            elif data_type in ['date', 'timestamp', 'timestamp without time zone', 'timestamp with time zone']:
                                val = '2026-01-13' if data_type == 'date' else '2026-01-13 00:00:00'
                            # For numeric columns
                            elif data_type in ['integer', 'bigint', 'smallint', 'numeric', 'decimal', 'real', 'double precision']:
                                val = 1 if 'frequency' in c.lower() or 'days' in c.lower() else 0
                            # For boolean columns
                            elif data_type in ['boolean']:
                                val = False
                            # For string/text columns
                            else:
                                val = f'{c}_default'
                    
                    # Get data type for formatting (use udt_name for JSONB)
                    if c in db_col_info:
                        is_nullable, data_type, udt_name = db_col_info[c]
                        # For JSONB columns, PostgreSQL returns data_type='USER-DEFINED' and udt_name='jsonb'
                        col_data_type = udt_name if data_type == 'USER-DEFINED' else data_type
                    else:
                        col_data_type = ''
                    row_vals.append(self.format_value(val, col_name=c, data_type=col_data_type))
                
                values_list.append(f"({', '.join(row_vals)})")
            
            if values_list:
                sql = f"INSERT INTO {table_name} ({', '.join(safe_cols)}) VALUES {', '.join(values_list)}"
                
                try:
                    cursor.execute(sql)
                    loaded += len(values_list)
                except Exception as e:
                    error_msg = str(e)
                    if len(error_msg) > 200:
                        error_msg = error_msg[:200]
                    logger.error(f"    ‚úó SQL Error in {table_name}: {error_msg}")
                    logger.error(f"    ‚úó SQL Query: {sql[:500]}...")
                    print(f"ERROR in {table_name}: {error_msg}")
                    print(f"SQL: {sql[:500]}...")
        
        return loaded
    
    # Alias for backwards compatibility
    def load_table(self, cursor, table_name, records):
        return self.load_table_ultra_fast(cursor, table_name, records)
    
    def load_table_worker(self, args):
        """Worker function for parallel table loading"""
        db_name, table_name, records = args
        
        try:
            # Create separate connection for this worker
            conn = psycopg2.connect(
                **self.db_config,
                dbname=db_name
            )
            conn.autocommit = True
            cursor = conn.cursor()
            
            # Load table using ultra-fast COPY method
            loaded = self.load_table_ultra_fast(cursor, table_name, records)
            
            cursor.close()
            conn.close()
            
            with self.progress_lock:
                logger.info(f"    ‚úì {db_name}.{table_name}: {loaded} records")
            
            return (table_name, loaded, None)
            
        except Exception as e:
            error_msg = str(e)
            if len(error_msg) > 200:
                error_msg = error_msg[:200]
            with self.progress_lock:
                logger.error(f"    ‚úó {db_name}.{table_name}: {error_msg}")
            return (table_name, 0, error_msg)
    
    def load_single_database(self, db_name, config):
        """Load data for single database with PARALLEL table loading"""
        data_file = config.get('data_file')
        if not data_file:
            with self.progress_lock:
                logger.info(f"  ‚äò Skipping: {db_name} (no data file)")
            return True
        
        data_path = self.root_path / data_file
        if not data_path.exists():
            with self.progress_lock:
                logger.warning(f"  ‚äò File not found: {data_path}")
            return True
        
        start_time = time.time()
        
        try:
            # Load JSON
            with open(data_path, 'r') as f:
                data = json.load(f)
            
            # Test database connection
            conn = psycopg2.connect(
                **self.db_config,
                dbname=db_name
            )
            conn.close()  # Just test connection
            
            # Prepare parallel table loading tasks
            table_tasks = []
            for table_name in data:
                records = data[table_name]
                if records and isinstance(records, list):
                    table_tasks.append((db_name, table_name, records))
            
            if not table_tasks:
                with self.progress_lock:
                    logger.info(f"  ‚äò No tables to load for {db_name}")
                return True
            
            # Load tables in parallel for maximum speed (up to 4 tables at once)
            max_table_workers = min(4, len(table_tasks))
            total_loaded = 0
            errors = []
            
            if max_table_workers > 1 and len(table_tasks) > 1:
                # Parallel table loading
                from concurrent.futures import ThreadPoolExecutor, as_completed
                
                with ThreadPoolExecutor(max_workers=max_table_workers) as executor:
                    futures = {executor.submit(self.load_table_worker, task): task for task in table_tasks}
                    
                    for future in as_completed(futures):
                        task = futures[future]
                        try:
                            table_name, loaded, error = future.result()
                            if error:
                                errors.append(f"{table_name}: {error}")
                            else:
                                total_loaded += loaded
                                with self.progress_lock:
                                    self.stats['records_loaded'] += loaded
                        except Exception as e:
                            table_name = task[1]
                            errors.append(f"{table_name}: {str(e)[:100]}")
                            with self.progress_lock:
                                logger.error(f"    ‚úó {db_name}.{table_name} worker failed: {str(e)[:100]}")
            else:
                # Sequential loading for single table or fallback
                conn = psycopg2.connect(
                    **self.db_config,
                    dbname=db_name
                )
                conn.autocommit = True
                cursor = conn.cursor()
                
                for db_name_task, table_name, records in table_tasks:
                    try:
                        loaded = self.load_table_ultra_fast(cursor, table_name, records)
                        total_loaded += loaded
                        with self.progress_lock:
                            logger.info(f"    ‚úì {db_name}.{table_name}: {loaded} records")
                            self.stats['records_loaded'] += loaded
                    except Exception as e:
                        error_msg = str(e)[:100]
                        errors.append(f"{table_name}: {error_msg}")
                        with self.progress_lock:
                            logger.error(f"    ‚úó {db_name}.{table_name}: {error_msg}")
                
            
            elapsed = time.time() - start_time
            with self.progress_lock:
                logger.info(f"  ‚Üí {db_name}: {len(table_tasks)} tables, {total_loaded} records ({elapsed:.1f}s)")
                self.stats['tables_loaded'] += len(table_tasks)
            
            if errors:
                with self.progress_lock:
                    for error in errors:
                        self.stats['errors'].append(f"{db_name}.{error}")
            
            return len(errors) == 0
            
        except Exception as e:
            with self.progress_lock:
                logger.error(f"  ‚úó {db_name}: {str(e)[:100]}")
                self.stats['errors'].append(f"{db_name}: {str(e)[:100]}")
            return False
    
    def load_data(self):
        """Load all generated data with DEPENDENCY-AWARE parallel processing"""
        self.log_section("STEP 4: Loading Data (DEPENDENCY-AWARE PARALLEL)")
        
        # Group databases by dependency tiers for ordered loading
        tier1_dbs = []  # Base data first
        tier2_dbs = []  # Core operations  
        tier3_dbs = []  # Everything else
        
        for db_name, config in DATABASES.items():
            data_file = config.get('data_file')
            if not data_file or not (self.root_path / data_file).exists():
                continue
                
            dependency_order = self.get_generator_dependencies(db_name)
            if dependency_order <= 2:  # Operations, MES
                tier1_dbs.append((db_name, config))
            elif dependency_order <= 5:  # ERP, WMS, Financial
                tier2_dbs.append((db_name, config))
            else:  # Everything else
                tier3_dbs.append((db_name, config))
        
        total_databases = len(tier1_dbs) + len(tier2_dbs) + len(tier3_dbs)
        completed = 0
        
        logger.info(f"  üìä Loading strategy: {len(tier1_dbs)} base + {len(tier2_dbs)} core + {len(tier3_dbs)} extended databases")
        
        # Tier 1: Load foundational data first (2 workers)
        if tier1_dbs:
            logger.info(f"  üèóÔ∏è  Tier 1: Loading {len(tier1_dbs)} foundational databases...")
            with ThreadPoolExecutor(max_workers=2) as executor:
                future_to_db = {executor.submit(self.load_single_database, db_name, config): db_name 
                               for db_name, config in tier1_dbs}
                
                for future in concurrent.futures.as_completed(future_to_db):
                    db_name = future_to_db[future]
                    completed += 1
                    try:
                        result = future.result()
                        if not result:
                            with self.progress_lock:
                                logger.warning(f"  Foundational database loading failed: {db_name}")
                    except Exception as exc:
                        with self.progress_lock:
                            logger.warning(f"  ‚úó Tier 1 loading exception for {db_name}: {exc}")
                            self.stats['errors'].append(f"Load exception {db_name}: {str(exc)[:100]}")
                    
                    with self.progress_lock:
                        logger.info(f"  üìà Progress: {completed}/{total_databases} ({completed/total_databases*100:.1f}%) - Tier 1")
        
        # Tier 2: Load core systems (3-4 workers)
        if tier2_dbs:
            workers = min(4, len(tier2_dbs))
            logger.info(f"  ‚öôÔ∏è  Tier 2: Loading {len(tier2_dbs)} core databases ({workers} workers)...")
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_db = {executor.submit(self.load_single_database, db_name, config): db_name 
                               for db_name, config in tier2_dbs}
                
                for future in concurrent.futures.as_completed(future_to_db):
                    db_name = future_to_db[future]
                    completed += 1
                    try:
                        result = future.result()
                        if not result:
                            with self.progress_lock:
                                logger.warning(f"  Core database loading failed: {db_name}")
                    except Exception as exc:
                        with self.progress_lock:
                            logger.warning(f"  ‚úó Tier 2 loading exception for {db_name}: {exc}")
                            self.stats['errors'].append(f"Load exception {db_name}: {str(exc)[:100]}")
                    
                    with self.progress_lock:
                        logger.info(f"  üìà Progress: {completed}/{total_databases} ({completed/total_databases*100:.1f}%) - Tier 2")
        
        # Tier 3: Load remaining databases rapidly (5-6 workers)
        if tier3_dbs:
            workers = min(6, len(tier3_dbs))
            logger.info(f"  ‚ö° Tier 3: Loading {len(tier3_dbs)} extended databases ({workers} workers)...")
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_db = {executor.submit(self.load_single_database, db_name, config): db_name 
                               for db_name, config in tier3_dbs}
                
                for future in concurrent.futures.as_completed(future_to_db):
                    db_name = future_to_db[future]
                    completed += 1
                    try:
                        result = future.result()
                        if not result:
                            with self.progress_lock:
                                logger.warning(f"  Extended database loading failed: {db_name}")
                    except Exception as exc:
                        with self.progress_lock:
                            logger.warning(f"  ‚úó Tier 3 loading exception for {db_name}: {exc}")
                            self.stats['errors'].append(f"Load exception {db_name}: {str(exc)[:100]}")
                    
                    with self.progress_lock:
                        logger.info(f"  üìà Progress: {completed}/{total_databases} ({completed/total_databases*100:.1f}%) - Tier 3")
        
        logger.info(f"\n‚úì Dependency-aware parallel loading completed: {self.stats['tables_loaded']} tables, {self.stats['records_loaded']:,} records")
        
        # Reset sequences after loading to ensure next inserts don't have duplicates
        self.reset_sequences()
        
        return True
        
        logger.info(f"\n‚úì Loaded {self.stats['tables_loaded']} tables, {self.stats['records_loaded']:,} records")
        
        # Reset sequences after loading to ensure next inserts don't have duplicates
        self.reset_sequences()
        
        return True
    
    def reset_sequences(self):
        """Reset all BIGSERIAL sequences with PARALLEL processing"""
        self.log_section("STEP 4.5: Resetting BIGSERIAL Sequences (PARALLEL)")
        
        # Get databases that actually exist
        databases_to_reset = [db_name for db_name in DATABASES.keys()]
        
        if not databases_to_reset:
            logger.info("  ‚ö† No databases found for sequence reset")
            return
        
        logger.info(f"  üìä Resetting sequences for {len(databases_to_reset)} databases in parallel...")
        
        total_sequences = 0
        completed = 0
        
        # Use ThreadPoolExecutor for parallel sequence reset (4 workers)
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_db = {executor.submit(self.reset_single_database_sequences, db_name): db_name 
                           for db_name in databases_to_reset}
            
            for future in concurrent.futures.as_completed(future_to_db):
                db_name = future_to_db[future]
                completed += 1
                
                try:
                    success, sequences_reset = future.result()
                    total_sequences += sequences_reset
                except Exception as exc:
                    with self.progress_lock:
                        logger.warning(f"  ‚úó Sequence reset exception for {db_name}: {exc}")
                
                with self.progress_lock:
                    logger.info(f"  üìà Sequence reset progress: {completed}/{len(databases_to_reset)} ({completed/len(databases_to_reset)*100:.1f}%)")
        
        logger.info(f"\n‚úì Parallel sequence reset completed: {total_sequences} sequences reset across {len(databases_to_reset)} databases")
    
    def reset_single_database_sequences(self, db_name):
        """Reset sequences for a single database (thread-safe)"""
        try:
            conn = psycopg2.connect(
                host=DB_HOST,
                port=DB_PORT,
                user=DB_USER,
                password=DB_PASSWORD,
                dbname=db_name,
                sslmode='require',
                connect_timeout=120
            )
            cursor = conn.cursor()
            
            # Get all sequences
            cursor.execute("""
                SELECT schemaname, sequencename 
                FROM pg_sequences 
                WHERE schemaname = 'public'
            """)
            sequences = cursor.fetchall()
            
            sequences_reset = 0
            for schema, seq_name in sequences:
                try:
                    # Reset sequence to start from 1
                    cursor.execute(f"SELECT setval('{seq_name}', 1, false)")
                    sequences_reset += 1
                except Exception as e:
                    with self.progress_lock:
                        logger.warning(f"  ‚ö† Failed to reset sequence {seq_name} in {db_name}: {str(e)[:80]}")
            
            conn.commit()
            cursor.close()
            conn.close()
            
            with self.progress_lock:
                logger.info(f"  ‚úì {db_name}: Reset {sequences_reset} sequences")
            
            return True, sequences_reset
            
        except Exception as e:
            with self.progress_lock:
                logger.error(f"  ‚úó Sequence reset failed for {db_name}: {e}")
            return False, 0
    
    # ========================================================================
    
    def execute(self):
        """Execute full setup pipeline"""
        logger.info("\n" + "="*80)
        logger.info("  GenIMS FULL SETUP - Database Creation to Data Loading")
        logger.info("  With Referential Integrity Validation & Registry Coordination")
        logger.info("="*80)
        logger.info(f"  Host: {DB_HOST}:{DB_PORT}")
        logger.info(f"  User: {DB_USER}")
        logger.info(f"  Root: {self.root_path}")
        
        success = True
        
        # Step 1: Create databases
        if not self.create_databases():
            success = False
        
        # Step 2: Load schemas
        if not self.load_schemas():
            success = False
        
        # Step 3a: Generate MASTER data first
        if not self.generate_master_data():
            success = False
        
        # Step 3b: Register master IDs with registry
        if not self.register_master_ids():
            success = False
        
        # Step 3c: Generate dependent data using registry
        if not self.generate_dependent_data():
            success = False
        
        # Step 3d: Validate referential integrity
        if not self.validate_referential_integrity():
            logger.warning("  ‚ö† Validation warnings - continuing anyway")
        
        # Step 4: Load data (and reset sequences)
        if not self.load_data():
            success = False
        
        # Summary
        self.print_summary(success)
        return success
    
    def print_summary(self, success):
        """Print execution summary"""
        elapsed = datetime.now() - self.start_time
        
        logger.info("\n" + "="*80)
        logger.info("  EXECUTION SUMMARY (PARALLEL OPTIMIZED)")
        logger.info("="*80)
        logger.info(f"  Databases Created:     {self.stats['databases_created']}")
        logger.info(f"  Schemas Loaded:        {self.stats['schemas_loaded']}")
        logger.info(f"  Data Generators Run:   {self.stats['data_generated']}")
        logger.info(f"  Tables Loaded:         {self.stats['tables_loaded']}")
        logger.info(f"  Records Loaded:        {self.stats['records_loaded']:,}")
        logger.info(f"  Errors:                {len(self.stats['errors'])}")
        logger.info(f"  Elapsed Time:          {elapsed}")
        
        if self.stats['errors']:
            logger.info(f"\n  Errors encountered:")
            for err in self.stats['errors'][:10]:
                logger.info(f"    - {err}")
            if len(self.stats['errors']) > 10:
                logger.info(f"    ... and {len(self.stats['errors'])-10} more")
        
        status = "‚úì SUCCESS" if success and not self.stats['errors'] else "‚ö† COMPLETED WITH ISSUES"
        logger.info(f"\n  Status: {status}")
        logger.info("="*80 + "\n")


def main():
    """Main entry point"""
    setup = GenIMSSetup()
    success = setup.execute()
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
